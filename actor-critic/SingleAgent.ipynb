{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/douglas/classes/RL/project/CS5180-project/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is a single agent actor-critic implementation.\n",
    "It is specifically designed to work with the four rooms environment, but it should work with any gym environment.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "# from tqdm import tqdm\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An OpenAI Gym environment for the Four Rooms domain. \n",
    "This is modified with permission from Christian Diamore's work.\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "# import typing\n",
    "from typing import Tuple, List, Dict, Union, Optional\n",
    "\n",
    "rooms = [\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "\n",
    "class FourRooms(gym.Env):\n",
    "    def __init__(self, grid: list = rooms, timeout=459):\n",
    "        # define the four room as a 2-D array for easy state space reference and\n",
    "        # visualization 0 represents an empty cell; 1 represents a wall cell\n",
    "\n",
    "        # NOTE: the origin for a 2-D numpy array is located at top-left\n",
    "        # while the origin for the FourRooms is at the bottom-left. The following\n",
    "        # codes performs the re-projection by reversing the rows\n",
    "        self.grid = np.array(list(reversed(grid)))\n",
    "\n",
    "        # define the action space\n",
    "        self.observation_space, self.action_space = (\n",
    "            spaces.MultiDiscrete(self.grid.shape, dtype=np.int8),\n",
    "            spaces.Box(low=-1, high=1, shape=(2,), dtype=int),\n",
    "        )\n",
    "\n",
    "        # define the start and goal state\n",
    "        self.agent_state = (0, 0)\n",
    "        self.start_state = (0, 0)\n",
    "        self.goal_state = (10, 10)\n",
    "\n",
    "        self.t = 0\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the agent's state to the start state [0, 0]\n",
    "        Return both the start state and reward\n",
    "        \"\"\"\n",
    "        # reset the agent state to be [0, 0]\n",
    "        self.agent_state, self.t = self.start_state, 0\n",
    "        return np.array(self.agent_state), {}\n",
    "\n",
    "    def is_wall(self, state: tuple):\n",
    "        return (\n",
    "            not self.observation_space.contains(np.array(state))\n",
    "            or self.grid[state] == 1\n",
    "        )\n",
    "\n",
    "    def step(self, act: tuple):\n",
    "        \"\"\"\n",
    "        :param act:\n",
    "            a tuple in {[1, 0], [-1, 0], [0, 1], [0, -1]} representing an\n",
    "            update to the state space\n",
    "\n",
    "        :returns:\n",
    "            :next_state: tuple\n",
    "                x, y integer coordinates of the agent's new state, i.e. (1, 1)\n",
    "            :reward: int\n",
    "                1 if the agent reached the goal, else 0\n",
    "            :done: bool.\n",
    "                whether the episode is done, either by hitting goal or timeout\n",
    "                on the episode\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "\n",
    "        # Compute the next state, and only update if we don't hit a wall\n",
    "        next_state = tuple(np.array(self.agent_state) + np.array(act))\n",
    "        if not self.is_wall(next_state): self.agent_state = next_state\n",
    "\n",
    "        # Reward the agent if it hits the goal\n",
    "        # Done if it was rewarded, or if it times out the episode\n",
    "        reward = 0.0 if self.agent_state == (10, 10) else -1.0\n",
    "        done = reward == 0.0 or (self.t >= self.timeout)\n",
    "\n",
    "        return np.array(self.agent_state), reward, done, False, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n",
    "\n",
    "\n",
    "class FourRoomsController(gym.ActionWrapper):\n",
    "    \"\"\"Map from a discrete action space into an actual movement in the grid.\"\"\"\n",
    "\n",
    "    def __init__(self, environment: gym.Env, controls: dict[int, tuple]):\n",
    "        assert all(\n",
    "            environment.action_space.contains(move)\n",
    "            for move in controls.values()\n",
    "        )\n",
    "\n",
    "        super().__init__(environment)\n",
    "        self.controls = controls\n",
    "        self.action_space = spaces.Discrete(len(controls))\n",
    "\n",
    "    def action(self, action):\n",
    "        return self.controls[action]\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        return -1 * self.controls[action]\n",
    "\n",
    "\n",
    "class FourRoomsRandomJitter(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    With probability p, the agent takes the correct direction.\n",
    "    With probability 1 - p, the agent takes one of the two perpendicular actions.\n",
    "\n",
    "    For example, if the correct action is \"LEFT\", then\n",
    "        - With probability 0.8, the agent takes action \"LEFT\";\n",
    "        - With probability 0.1, the agent takes action \"UP\";\n",
    "        - With probability 0.1, the agent takes action \"DOWN\".\n",
    "    \"\"\"\n",
    "    def __init__(self, environment: gym.Env, p: float = 0.8):\n",
    "        super().__init__(environment)\n",
    "        self.p = p\n",
    "\n",
    "    @staticmethod\n",
    "    def perpendicular_action(action):\n",
    "        return np.random.choice([2, 3] if action in [0, 1] else [0, 1], 1)[0]\n",
    "\n",
    "    def action(self, action):\n",
    "        return (\n",
    "            action\n",
    "            if np.random.uniform() < self.p else\n",
    "            self.perpendicular_action(action)\n",
    "        )\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        pass\n",
    "\n",
    "controls={\n",
    "    0: (-1, 0),  # 'LEFT'\n",
    "    1: (1, 0),  # 'RIGHT'\n",
    "    2: (0, -1),  # 'DOWN'\n",
    "    3: (0, 1),  # 'UP'\n",
    "}\n",
    "\n",
    "timeout = 1000\n",
    "four_rooms_env_no_jitter = FourRoomsController(FourRooms(rooms, timeout=timeout), controls=controls)\n",
    "\n",
    "four_rooms_env_jitter = FourRoomsRandomJitter(\n",
    "    FourRoomsController(FourRooms(rooms, timeout=timeout), controls=controls), p=0.8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticSingleAgent(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple, but flexible, implementation of the actor-critic algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, lr=3e-4):\n",
    "        super(ActorCriticSingleAgent, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        # estimate the value function\n",
    "        self.critic1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.critic2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # estimate the policy distribution\n",
    "        self.actor1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.actor2 = nn.Linear(hidden_size, num_actions)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Do inference to calculate the action probabilities and the state value.\n",
    "        \"\"\"\n",
    "        critic_out = F.relu(self.critic1(state))\n",
    "        critic_out = self.critic2(critic_out)\n",
    "        \n",
    "        actor_out = F.relu(self.actor1(state))\n",
    "        # softmax effectively generates a probability for each of our output options\n",
    "        actor_out = F.softmax(self.actor2(actor_out), dim=0)\n",
    "\n",
    "        return critic_out, actor_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(arr_list, legend_list, color_list, ylabel, fig_title):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        arr_list (list): list of results arrays to plot\n",
    "        legend_list (list): list of legends corresponding to each result array\n",
    "        color_list (list): list of color corresponding to each result array\n",
    "        ylabel (string): label of the Y axis\n",
    "\n",
    "        Note that, make sure the elements in the arr_list, legend_list and color_list are associated with each other correctly.\n",
    "        Do not forget to change the ylabel for different plots.\n",
    "    \"\"\"\n",
    "    # set the figure type\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # PLEASE NOTE: Change the labels for different plots\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "\n",
    "    # ploth results\n",
    "    h_list = []\n",
    "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
    "        # compute the standard error\n",
    "        arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])\n",
    "        # plot the mean\n",
    "        h, = ax.plot(range(arr.shape[1]), arr.mean(axis=0), color=color, label=legend)\n",
    "        # plot the confidence band\n",
    "        arr_err *= 1.96\n",
    "        ax.fill_between(range(arr.shape[1]), arr.mean(axis=0) - arr_err, arr.mean(axis=0) + arr_err, alpha=0.3,\n",
    "                        color=color)\n",
    "        # save the plot handle\n",
    "        h_list.append(h)\n",
    "\n",
    "    # plot legends\n",
    "    ax.set_title(f\"{fig_title}\")\n",
    "    ax.legend(handles=h_list)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CTD0:\n",
    "    def __init__(self, env, obs_size, hidden_size, output_size, learning_rate, num_episodes, num_steps, gamma) -> None:\n",
    "        self.env = env\n",
    "        self.obs_size = obs_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_steps = num_steps\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.model = ActorCriticSingleAgent(self.obs_size * self.env.observation_space.shape[0] + 1, self.output_size, self.hidden_size, self.learning_rate)\n",
    "    \n",
    "    def _get_action(self, actor_act_pred):\n",
    "        # Sample an action according the probs the network just output.\n",
    "        action = np.random.choice(self.output_size, p=np.squeeze(actor_act_pred))\n",
    "        return action\n",
    "\n",
    "    def _obs_to_tensor(self, obs):\n",
    "        state_tensor = torch.flatten(F.one_hot(torch.tensor(obs, dtype=torch.int64), num_classes=self.obs_size))\n",
    "        # add a bias bit with a value of 1 in front of the one hot vector\n",
    "        state_tensor = torch.cat((torch.tensor([1.0]), state_tensor))\n",
    "        return state_tensor\n",
    "\n",
    "    def train(self):\n",
    "        # Use Adam optimizer for the actor-critic because it should converge faster than SGD and generalization may not be super important\n",
    "        ac_optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        # all episode length\n",
    "        all_lengths = []\n",
    "        episode_rewards = []\n",
    "        # average episode length\n",
    "        average_lengths = []\n",
    "        all_entropies = []\n",
    "        all_losses = []\n",
    "\n",
    "        for episode in range(self.num_episodes):\n",
    "            rewards = []\n",
    "            done = False\n",
    "            i = 1\n",
    "\n",
    "            state, _ = self.env.reset()\n",
    "            steps = 0\n",
    "            # enable n step actor critic. \n",
    "            while not done:\n",
    "                state_tensor = self._obs_to_tensor(state)\n",
    "                critic_td_error, actor_act_pred = self.model.forward(state_tensor.float())\n",
    "                # drop the tensor dimension and computational graph info\n",
    "                detached_act_pred = actor_act_pred.detach().numpy()\n",
    "                action = self._get_action(detached_act_pred)\n",
    "\n",
    "                # Calculate the log probability of the action we've taken\n",
    "                log_prob = torch.distributions.Categorical(actor_act_pred).log_prob(torch.tensor(action))\n",
    "\n",
    "                # Calculate the entropy/ uncertainty of the policy term. This is used to encourage exploration\n",
    "                # entropy = -np.sum(np.mean(detached_act_pred) * np.log(detached_act_pred))\n",
    "                new_state, reward, done, _, _ = self.env.step(action)\n",
    "                steps += 1\n",
    "\n",
    "                state = new_state\n",
    "                \n",
    "                state_tensor = self._obs_to_tensor(state)\n",
    "                if not done:\n",
    "                  q_val_tensor, _ = self.model.forward(state_tensor.float())\n",
    "                else:\n",
    "                    q_val_tensor = torch.tensor(0)\n",
    "                    all_lengths.append(steps)\n",
    "                    average_lengths.append(np.mean(all_lengths[-10:]))\n",
    "                td_error = reward + self.gamma * q_val_tensor - critic_td_error\n",
    "        \n",
    "                # update actor critic\n",
    "                actor_loss = -log_prob * (reward + self.gamma * q_val_tensor.item() - critic_td_error.item())\n",
    "                actor_loss *= i\n",
    "                critic_loss = 0.5 * td_error * critic_td_error ** 2\n",
    "                critic_loss *= i\n",
    "\n",
    "                ac_loss = actor_loss + critic_loss #+ 0.001 * entropy\n",
    "\n",
    "                ac_optimizer.zero_grad()\n",
    "                ac_loss.backward()\n",
    "                ac_optimizer.step()\n",
    "                # all_entropies.append(entropy)\n",
    "                all_losses.append(ac_loss.detach().numpy())\n",
    "                i *= self.gamma\n",
    "            if done:\n",
    "              episode_rewards.append(np.sum(rewards))\n",
    "            if episode % 5 == 0:\n",
    "                sys.stdout.write(\"\\nLoss: \" + str(all_losses[-1]) + \"\\nCurrent State: \" + str(state))\n",
    "                # Where total length is the number of steps taken in the episode and average length is average steps in all episodes seen \n",
    "                sys.stdout.write(\"\\nepisode: {}, total length: {}, average length of prev 10: {} \\n\".format(episode, steps, average_lengths[-1]))\n",
    "            \n",
    "        return all_lengths, average_lengths, all_entropies, all_losses, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: [-0.0004537]\n",
      "Current State: [1 3]\n",
      "episode: 0, total length: 1000, average length of prev 10: 1000.0 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "These results are generated with a2c reward of 1 on episode completion and -0.5 on every step\n",
    "\"\"\"\n",
    "hidden_size = 64\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Constants\n",
    "gamma = 0.99\n",
    "num_steps = 1\n",
    "max_episodes = 1000\n",
    "state_height = 11\n",
    "\n",
    "# 22 inputs. 11 for the current state and 11 for the next state. To one hot encode our integer values.\n",
    "output_size = four_rooms_env_no_jitter.action_space.n\n",
    "many_run_lengths, many_run_average_lengths, many_run_entropies, many_run_losses, many_run_rewards = [], [], [], [], []\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for i in range(5):\n",
    "    agent = A2CTD0(four_rooms_env_jitter, state_height, hidden_size, output_size, learning_rate, num_episodes=max_episodes, num_steps=num_steps, gamma=gamma)\n",
    "    all_lengths, average_lengths, all_entropies, all_losses, episode_rewards = agent.train()\n",
    "\n",
    "    many_run_average_lengths.append(average_lengths)\n",
    "\n",
    "    # rolling_avg_entropies = pd.Series(all_entropies).rolling(10).mean()\n",
    "    # many_run_entropies.append(rolling_avg_entropies)\n",
    "    \n",
    "    rolling_avg_losses = pd.Series(all_losses).rolling(10).mean()\n",
    "    many_run_losses.append(rolling_avg_losses)\n",
    "\n",
    "    many_run_rewards.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_curves(np.array(many_run_lengths), [\"A2C\"], [\"blue\"], \"Episode Length\", \"A2C Episode Length\")\n",
    "plot_curves([np.array(many_run_average_lengths)], [\"A2C\"], [\"blue\"], \"Average Episode Length\", \"A2C Average Episode Length\")\n",
    "plot_curves([np.array(many_run_entropies)], [\"A2C\"], [\"blue\"], \"Entropy\", \"A2C Entropy\")\n",
    "plot_curves([np.array(many_run_losses)], [\"A2C\"], [\"blue\"], \"Loss\", \"A2C Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e95bd2e9b31e81c6acdd4fdb88c8368cef1b094bdbe6f4c46c83eec4428ed37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
