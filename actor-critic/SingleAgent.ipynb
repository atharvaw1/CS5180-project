{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a single agent actor-critic implementation.\n",
    "It is specifically designed to work with the four rooms environment, but it should work with any gym environment.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "# from tqdm import tqdm\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An OpenAI Gym environment for the Four Rooms domain. \n",
    "This is modified with permission from Christian Diamore's work.\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "# set the torch device to be GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# import typing\n",
    "from typing import Tuple, List, Dict, Union, Optional\n",
    "\n",
    "rooms = [\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "\n",
    "class FourRooms(gym.Env):\n",
    "    def __init__(self, grid: list = rooms, timeout=459):\n",
    "        # define the four room as a 2-D array for easy state space reference and\n",
    "        # visualization 0 represents an empty cell; 1 represents a wall cell\n",
    "\n",
    "        # NOTE: the origin for a 2-D numpy array is located at top-left\n",
    "        # while the origin for the FourRooms is at the bottom-left. The following\n",
    "        # codes performs the re-projection by reversing the rows\n",
    "        self.grid = np.array(list(reversed(grid)))\n",
    "\n",
    "        # define the action space\n",
    "        self.observation_space, self.action_space = (\n",
    "            spaces.MultiDiscrete(self.grid.shape, dtype=np.int8),\n",
    "            spaces.Box(low=-1, high=1, shape=(2,), dtype=int),\n",
    "        )\n",
    "\n",
    "        # define the start and goal state\n",
    "        self.agent_state = (0, 0)\n",
    "        self.start_state = (0, 0)\n",
    "        self.goal_state = (10, 10)\n",
    "\n",
    "        self.t = 0\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the agent's state to the start state [0, 0]\n",
    "        Return both the start state and reward\n",
    "        \"\"\"\n",
    "        # reset the agent state to be [0, 0]\n",
    "        self.agent_state, self.t = self.start_state, 0\n",
    "        return np.array(self.agent_state), {}\n",
    "\n",
    "    def is_wall(self, state: tuple):\n",
    "        return (\n",
    "            not self.observation_space.contains(np.array(state))\n",
    "            or self.grid[state] == 1\n",
    "        )\n",
    "\n",
    "    def step(self, act: tuple):\n",
    "        \"\"\"\n",
    "        :param act:\n",
    "            a tuple in {[1, 0], [-1, 0], [0, 1], [0, -1]} representing an\n",
    "            update to the state space\n",
    "\n",
    "        :returns:\n",
    "            :next_state: tuple\n",
    "                x, y integer coordinates of the agent's new state, i.e. (1, 1)\n",
    "            :reward: int\n",
    "                1 if the agent reached the goal, else 0\n",
    "            :done: bool.\n",
    "                whether the episode is done, either by hitting goal or timeout\n",
    "                on the episode\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "\n",
    "        # Compute the next state, and only update if we don't hit a wall\n",
    "        next_state = tuple(np.array(self.agent_state) + np.array(act))\n",
    "        if not self.is_wall(next_state): self.agent_state = next_state\n",
    "\n",
    "        # Reward the agent if it hits the goal\n",
    "        # Done if it was rewarded, or if it times out the episode\n",
    "        reward = 0.0 if self.agent_state == (10, 10) else -1.0\n",
    "        done = reward == 0.0 or (self.t >= self.timeout)\n",
    "\n",
    "        return np.array(self.agent_state), reward, done, False, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n",
    "\n",
    "\n",
    "class FourRoomsController(gym.ActionWrapper):\n",
    "    \"\"\"Map from a discrete action space into an actual movement in the grid.\"\"\"\n",
    "\n",
    "    def __init__(self, environment: gym.Env, controls: dict[int, tuple]):\n",
    "        assert all(\n",
    "            environment.action_space.contains(move)\n",
    "            for move in controls.values()\n",
    "        )\n",
    "\n",
    "        super().__init__(environment)\n",
    "        self.controls = controls\n",
    "        self.action_space = spaces.Discrete(len(controls))\n",
    "\n",
    "    def action(self, action):\n",
    "        return self.controls[action]\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        return -1 * self.controls[action]\n",
    "\n",
    "\n",
    "class FourRoomsRandomJitter(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    With probability p, the agent takes the correct direction.\n",
    "    With probability 1 - p, the agent takes one of the two perpendicular actions.\n",
    "\n",
    "    For example, if the correct action is \"LEFT\", then\n",
    "        - With probability 0.8, the agent takes action \"LEFT\";\n",
    "        - With probability 0.1, the agent takes action \"UP\";\n",
    "        - With probability 0.1, the agent takes action \"DOWN\".\n",
    "    \"\"\"\n",
    "    def __init__(self, environment: gym.Env, p: float = 0.8):\n",
    "        super().__init__(environment)\n",
    "        self.p = p\n",
    "\n",
    "    @staticmethod\n",
    "    def perpendicular_action(action):\n",
    "        return np.random.choice([2, 3] if action in [0, 1] else [0, 1], 1)[0]\n",
    "\n",
    "    def action(self, action):\n",
    "        return (\n",
    "            action\n",
    "            if np.random.uniform() < self.p else\n",
    "            self.perpendicular_action(action)\n",
    "        )\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        pass\n",
    "\n",
    "controls={\n",
    "    0: (-1, 0),  # 'LEFT'\n",
    "    1: (1, 0),  # 'RIGHT'\n",
    "    2: (0, -1),  # 'DOWN'\n",
    "    3: (0, 1),  # 'UP'\n",
    "}\n",
    "\n",
    "timeout = 500\n",
    "four_rooms_env_no_jitter = FourRoomsController(FourRooms(rooms, timeout=timeout), controls=controls)\n",
    "\n",
    "four_rooms_env_jitter = FourRoomsRandomJitter(\n",
    "    FourRoomsController(FourRooms(rooms, timeout=timeout), controls=controls), p=0.8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticSingleAgent(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple, but flexible, implementation of the actor-critic algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, lr=3e-4):\n",
    "        super(ActorCriticSingleAgent, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        # estimate the value function\n",
    "        self.critic1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.critic2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # estimate the policy distribution\n",
    "        self.actor1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.actor2 = nn.Linear(hidden_size, num_actions)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Do inference to calculate the action probabilities and the state value.\n",
    "        \"\"\"\n",
    "        critic_out = F.relu(self.critic1(state))\n",
    "        critic_out = self.critic2(critic_out)\n",
    "        \n",
    "        actor_out = F.relu(self.actor1(state))\n",
    "        # softmax effectively generates a probability for each of our output options\n",
    "        actor_out = F.softmax(self.actor2(actor_out), dim=0)\n",
    "\n",
    "        return critic_out, actor_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(arr_list, legend_list, color_list, ylabel, fig_title):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        arr_list (list): list of results arrays to plot\n",
    "        legend_list (list): list of legends corresponding to each result array\n",
    "        color_list (list): list of color corresponding to each result array\n",
    "        ylabel (string): label of the Y axis\n",
    "\n",
    "        Note that, make sure the elements in the arr_list, legend_list and color_list are associated with each other correctly.\n",
    "        Do not forget to change the ylabel for different plots.\n",
    "    \"\"\"\n",
    "    # set the figure type\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # PLEASE NOTE: Change the labels for different plots\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "\n",
    "    # ploth results\n",
    "    h_list = []\n",
    "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
    "        # compute the standard error\n",
    "        arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])\n",
    "        # plot the mean\n",
    "        h, = ax.plot(range(arr.shape[1]), arr.mean(axis=0), color=color, label=legend)\n",
    "        # plot the confidence band\n",
    "        arr_err *= 1.96\n",
    "        ax.fill_between(range(arr.shape[1]), arr.mean(axis=0) - arr_err, arr.mean(axis=0) + arr_err, alpha=0.3,\n",
    "                        color=color)\n",
    "        # save the plot handle\n",
    "        h_list.append(h)\n",
    "\n",
    "    # plot legends\n",
    "    ax.set_title(f\"{fig_title}\")\n",
    "    ax.legend(handles=h_list)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CTD0:\n",
    "    def __init__(self, env, obs_size, hidden_size, output_size, learning_rate, num_episodes, num_steps, gamma) -> None:\n",
    "        self.env = env\n",
    "        self.obs_size = obs_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_steps = num_steps\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.model = ActorCriticSingleAgent(self.obs_size * self.env.observation_space.shape[0] + 1, self.output_size, self.hidden_size, self.learning_rate)\n",
    "    \n",
    "    def _get_action(self, actor_act_pred):\n",
    "        # Sample an action according the probs the network just output.\n",
    "        action = np.random.choice(self.output_size, p=np.squeeze(actor_act_pred))\n",
    "        return action\n",
    "\n",
    "    def _obs_to_tensor(self, obs):\n",
    "        state_tensor = torch.flatten(F.one_hot(torch.tensor(obs, dtype=torch.int64), num_classes=self.obs_size))\n",
    "        # add a bias bit with a value of 1 in front of the one hot vector\n",
    "        state_tensor = torch.cat((torch.tensor([1.0]), state_tensor))\n",
    "        return state_tensor\n",
    "\n",
    "    def _show_episode_results(self, episode, steps, state, all_losses, average_lengths):\n",
    "        if episode % 10 == 0:\n",
    "            # make an array of the value of each state currently\n",
    "            # make an empty array for the values of each state of shape 10x10\n",
    "            state_values = np.zeros((10, 10))\n",
    "            for i in range(1, 11):\n",
    "                for j in range(1, 11):\n",
    "                    state_tensor = self._obs_to_tensor([i, j])\n",
    "                    value, _ = self.model.forward(state_tensor.float())\n",
    "                    state_values[i-1][j-1] = value.detach().numpy()\n",
    "            # show the value of each state with matplot lib\n",
    "            plt.imshow(state_values)\n",
    "            plt.show()\n",
    "                # sys.stdout.write(\"\\nState: \" + str(i) + \" Value: \" + str(state_values))\n",
    "            sys.stdout.write(\"\\nLoss: \" + str(all_losses[-1]) + \"\\nCurrent State: \" + str(state))\n",
    "            # Where total length is the number of steps taken in the episode and average length is average steps in all episodes seen \n",
    "            sys.stdout.write(\"\\nepisode: {}, total length: {}, average length of prev 10: {} \\n\".format(episode, steps, average_lengths[-1]))\n",
    "\n",
    "    def train(self):\n",
    "        # Use Adam optimizer for the actor-critic because it should converge faster than SGD and generalization may not be super important\n",
    "        ac_optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        # all episode length\n",
    "        all_lengths = []\n",
    "        episode_rewards = []\n",
    "        # average episode length\n",
    "        average_lengths = []\n",
    "        all_entropies = []\n",
    "        all_losses = []\n",
    "\n",
    "        for episode in range(self.num_episodes):\n",
    "            rewards = []\n",
    "            done = False\n",
    "            i = 1\n",
    "\n",
    "            state, _ = self.env.reset()\n",
    "            steps = 0\n",
    "            # enable n step actor critic. \n",
    "            while not done:\n",
    "                state_tensor = self._obs_to_tensor(state)\n",
    "                critic_td_error, actor_act_pred = self.model.forward(state_tensor.float())\n",
    "                # drop the tensor dimension and computational graph info\n",
    "                detached_act_pred = actor_act_pred.detach().numpy()\n",
    "                action = self._get_action(detached_act_pred)\n",
    "\n",
    "                # Calculate the log probability of the action we've taken\n",
    "                log_prob = torch.distributions.Categorical(actor_act_pred).log_prob(torch.tensor(action))\n",
    "\n",
    "                # Calculate the entropy/ uncertainty of the policy term. This is used to encourage exploration\n",
    "                entropy = -np.sum(np.mean(detached_act_pred) * np.log(detached_act_pred))\n",
    "                new_state, reward, done, _, _ = self.env.step(action)\n",
    "                steps += 1\n",
    "\n",
    "                state = new_state\n",
    "                \n",
    "                state_tensor = self._obs_to_tensor(state)\n",
    "                if not done:\n",
    "                  q_val_tensor, _ = self.model.forward(state_tensor.float())\n",
    "                else:\n",
    "                    q_val_tensor = torch.tensor(0)\n",
    "                    all_lengths.append(steps)\n",
    "                    average_lengths.append(np.mean(all_lengths[-10:]))\n",
    "                td_error = reward + self.gamma * q_val_tensor - critic_td_error\n",
    "        \n",
    "                # update actor critic\n",
    "                actor_loss = -log_prob * (reward + self.gamma * q_val_tensor.item() - critic_td_error.item())\n",
    "                actor_loss *= i\n",
    "                critic_loss = 0.5 * td_error ** 2\n",
    "                critic_loss *= i\n",
    "\n",
    "                ac_loss = actor_loss + critic_loss + 0.001 * entropy\n",
    "\n",
    "                ac_optimizer.zero_grad()\n",
    "                ac_loss.backward()\n",
    "                ac_optimizer.step()\n",
    "                # all_entropies.append(entropy)\n",
    "                all_losses.append(ac_loss.detach().numpy())\n",
    "                i *= self.gamma\n",
    "            if done:\n",
    "              episode_rewards.append(np.sum(rewards))\n",
    "            self._show_episode_results(episode, steps, state, all_losses, average_lengths)\n",
    "            \n",
    "        return all_lengths, average_lengths, all_entropies, all_losses, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[1;32m     18\u001b[0m     agent \u001b[39m=\u001b[39m A2CTD0(four_rooms_env_jitter, state_height, hidden_size, output_size, learning_rate, num_episodes\u001b[39m=\u001b[39mmax_episodes, num_steps\u001b[39m=\u001b[39mnum_steps, gamma\u001b[39m=\u001b[39mgamma)\n\u001b[0;32m---> 19\u001b[0m     all_lengths, average_lengths, all_entropies, all_losses, episode_rewards \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     21\u001b[0m     many_run_average_lengths\u001b[39m.\u001b[39mappend(average_lengths)\n\u001b[1;32m     23\u001b[0m     \u001b[39m# rolling_avg_entropies = pd.Series(all_entropies).rolling(10).mean()\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[39m# many_run_entropies.append(rolling_avg_entropies)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [19], line 46\u001b[0m, in \u001b[0;36mA2CTD0.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m     45\u001b[0m     state_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obs_to_tensor(state)\n\u001b[0;32m---> 46\u001b[0m     critic_td_error, actor_act_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward(state_tensor\u001b[39m.\u001b[39;49mfloat())\n\u001b[1;32m     47\u001b[0m     \u001b[39m# drop the tensor dimension and computational graph info\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     detached_act_pred \u001b[39m=\u001b[39m actor_act_pred\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn [17], line 26\u001b[0m, in \u001b[0;36mActorCriticSingleAgent.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     24\u001b[0m actor_out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor1(state))\n\u001b[1;32m     25\u001b[0m \u001b[39m# softmax effectively generates a probability for each of our output options\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m actor_out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49msoftmax(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor2(actor_out), dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m critic_out, actor_out\n",
      "File \u001b[0;32m~/classes/RL/project/CS5180-project/venv/lib/python3.10/site-packages/torch/nn/functional.py:1841\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1839\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1840\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1841\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim)\n\u001b[1;32m   1842\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1843\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/classes/RL/project/CS5180-project/venv/lib/python3.10/site-packages/torch/fx/traceback.py:57\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m current_stack\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     55\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[39m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m traceback\u001b[39m.\u001b[39;49mformat_stack()\n",
      "File \u001b[0;32m/usr/lib/python3.10/traceback.py:213\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     f \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39m_getframe()\u001b[39m.\u001b[39mf_back\n\u001b[0;32m--> 213\u001b[0m \u001b[39mreturn\u001b[39;00m format_list(extract_stack(f, limit\u001b[39m=\u001b[39;49mlimit))\n",
      "File \u001b[0;32m/usr/lib/python3.10/traceback.py:227\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     f \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39m_getframe()\u001b[39m.\u001b[39mf_back\n\u001b[0;32m--> 227\u001b[0m stack \u001b[39m=\u001b[39m StackSummary\u001b[39m.\u001b[39;49mextract(walk_stack(f), limit\u001b[39m=\u001b[39;49mlimit)\n\u001b[1;32m    228\u001b[0m stack\u001b[39m.\u001b[39mreverse()\n\u001b[1;32m    229\u001b[0m \u001b[39mreturn\u001b[39;00m stack\n",
      "File \u001b[0;32m/usr/lib/python3.10/traceback.py:379\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    376\u001b[0m     result\u001b[39m.\u001b[39mappend(FrameSummary(\n\u001b[1;32m    377\u001b[0m         filename, lineno, name, lookup_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39mlocals\u001b[39m\u001b[39m=\u001b[39mf_locals))\n\u001b[1;32m    378\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m fnames:\n\u001b[0;32m--> 379\u001b[0m     linecache\u001b[39m.\u001b[39;49mcheckcache(filename)\n\u001b[1;32m    380\u001b[0m \u001b[39m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[0;32m/usr/lib/python3.10/linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[39mcontinue\u001b[39;00m   \u001b[39m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     stat \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(fullname)\n\u001b[1;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     cache\u001b[39m.\u001b[39mpop(filename, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "These results are generated with a2c reward of 1 on episode completion and -0.5 on every step\n",
    "\"\"\"\n",
    "hidden_size = 64\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Constants\n",
    "gamma = 0.99\n",
    "num_steps = 1\n",
    "max_episodes = 1000\n",
    "state_height = 11\n",
    "\n",
    "# 22 inputs. 11 for the current state and 11 for the next state. To one hot encode our integer values.\n",
    "output_size = four_rooms_env_no_jitter.action_space.n\n",
    "many_run_lengths, many_run_average_lengths, many_run_entropies, many_run_losses, many_run_rewards = [], [], [], [], []\n",
    "\n",
    "for i in range(5):\n",
    "    agent = A2CTD0(four_rooms_env_no_jitter, state_height, hidden_size, output_size, learning_rate, num_episodes=max_episodes, num_steps=num_steps, gamma=gamma)\n",
    "    all_lengths, average_lengths, all_entropies, all_losses, episode_rewards = agent.train()\n",
    "\n",
    "    many_run_average_lengths.append(average_lengths)\n",
    "\n",
    "    # rolling_avg_entropies = pd.Series(all_entropies).rolling(10).mean()\n",
    "    # many_run_entropies.append(rolling_avg_entropies)\n",
    "    \n",
    "    rolling_avg_losses = pd.Series(all_losses).rolling(10).mean()\n",
    "    many_run_losses.append(rolling_avg_losses)\n",
    "\n",
    "    many_run_rewards.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_curves(np.array(many_run_lengths), [\"A2C\"], [\"blue\"], \"Episode Length\", \"A2C Episode Length\")\n",
    "plot_curves([np.array(many_run_average_lengths)], [\"A2C\"], [\"blue\"], \"Average Episode Length\", \"A2C Average Episode Length\")\n",
    "plot_curves([np.array(many_run_entropies)], [\"A2C\"], [\"blue\"], \"Entropy\", \"A2C Entropy\")\n",
    "plot_curves([np.array(many_run_losses)], [\"A2C\"], [\"blue\"], \"Loss\", \"A2C Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(A2CTD0):\n",
    "    def __init__(self, env, obs_size, hidden_size, output_size, learning_rate, num_episodes, num_steps, gamma) -> None:\n",
    "        super().__init__(env, obs_size, hidden_size, output_size, learning_rate, num_episodes, num_steps, gamma)\n",
    "\n",
    "    def train(self):\n",
    "        # Use Adam optimizer for the actor-critic because it should converge faster than SGD and generalization may not be super important\n",
    "        ac_optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        # all episode length\n",
    "        all_lengths = []\n",
    "        episode_rewards = []\n",
    "        # average episode length\n",
    "        average_lengths = []\n",
    "        all_entropies = []\n",
    "        all_losses = []\n",
    "\n",
    "        for episode in range(self.num_episodes):\n",
    "            rewards = []\n",
    "            done = False\n",
    "            i = 1\n",
    "\n",
    "            state, _ = self.env.reset()\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                state_tensor = self._obs_to_tensor(state)\n",
    "                critic_td_error, actor_act_pred = self.model.forward(state_tensor.float())\n",
    "                # drop the tensor dimension and computational graph info\n",
    "                detached_act_pred = actor_act_pred.detach().numpy()\n",
    "                action = self._get_action(detached_act_pred)\n",
    "\n",
    "                # Calculate the log probability of the action we've taken\n",
    "                log_prob = torch.distributions.Categorical(actor_act_pred).log_prob(torch.tensor(action))\n",
    "\n",
    "                # Calculate the entropy/ uncertainty of the policy term. This is used to encourage exploration\n",
    "                entropy = -np.sum(np.mean(detached_act_pred) * np.log(detached_act_pred))\n",
    "                new_state, reward, done, _, _ = self.env.step(action)\n",
    "                steps += 1\n",
    "\n",
    "                state = new_state\n",
    "                \n",
    "                state_tensor = self._obs_to_tensor(state)\n",
    "                if not done:\n",
    "                  q_val_tensor, _ = self.model.forward(state_tensor.float())\n",
    "                else:\n",
    "                    q_val_tensor = torch.tensor(0)\n",
    "                    all_lengths.append(steps)\n",
    "                    average_lengths.append(np.mean(all_lengths[-10:]))\n",
    "                td_error = reward + self.gamma * q_val_tensor - critic_td_error\n",
    "        \n",
    "                # update actor critic\n",
    "                actor_loss = -log_prob * (reward + self.gamma * q_val_tensor.item() - critic_td_error.item())\n",
    "                actor_loss *= i\n",
    "                critic_loss = 0.5 * td_error ** 2\n",
    "                critic_loss *= i\n",
    "\n",
    "                ac_loss = actor_loss + critic_loss + 0.001 * entropy\n",
    "\n",
    "                ac_optimizer.zero_grad()\n",
    "                ac_loss.backward()\n",
    "                ac_optimizer.step()\n",
    "                # all_entropies.append(entropy)\n",
    "                all_losses.append(ac_loss.detach().numpy())\n",
    "                i *= self.gamma\n",
    "            if done:\n",
    "              episode_rewards.append(np.sum(rewards))\n",
    "            \n",
    "        return all_lengths, average_lengths, all_entropies, all_losses, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: [0.00135002]\n",
      "Current State: [5 9]\n",
      "episode: 0, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.0016259]\n",
      "Current State: [10 10]\n",
      "episode: 2, total length: 702, average length of prev 10: 771.3333333333334 \n",
      "\n",
      "Loss: [0.00147558]\n",
      "Current State: [10 10]\n",
      "episode: 4, total length: 912, average length of prev 10: 845.2 \n",
      "\n",
      "Loss: [0.00139765]\n",
      "Current State: [1 9]\n",
      "episode: 6, total length: 1000, average length of prev 10: 889.4285714285714 \n",
      "\n",
      "Loss: [0.00143008]\n",
      "Current State: [7 0]\n",
      "episode: 8, total length: 1000, average length of prev 10: 914.0 \n",
      "\n",
      "Loss: [0.0014584]\n",
      "Current State: [1 7]\n",
      "episode: 10, total length: 1000, average length of prev 10: 922.6 \n",
      "\n",
      "Loss: [0.00163831]\n",
      "Current State: [1 0]\n",
      "episode: 12, total length: 1000, average length of prev 10: 991.2 \n",
      "\n",
      "Loss: [0.00176119]\n",
      "Current State: [3 0]\n",
      "episode: 14, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.00194415]\n",
      "Current State: [0 0]\n",
      "episode: 16, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.00203204]\n",
      "Current State: [2 1]\n",
      "episode: 18, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.00244131]\n",
      "Current State: [0 3]\n",
      "episode: 20, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.01018956]\n",
      "Current State: [10 10]\n",
      "episode: 22, total length: 863, average length of prev 10: 986.3 \n",
      "\n",
      "Loss: [0.00371654]\n",
      "Current State: [3 7]\n",
      "episode: 24, total length: 1000, average length of prev 10: 925.3 \n",
      "\n",
      "Loss: [0.03740943]\n",
      "Current State: [10 10]\n",
      "episode: 26, total length: 771, average length of prev 10: 868.1 \n",
      "\n",
      "Loss: [0.07524958]\n",
      "Current State: [10 10]\n",
      "episode: 28, total length: 728, average length of prev 10: 835.6 \n",
      "\n",
      "Loss: [0.89124495]\n",
      "Current State: [10 10]\n",
      "episode: 30, total length: 485, average length of prev 10: 698.9 \n",
      "\n",
      "Loss: [40.884197]\n",
      "Current State: [10 10]\n",
      "episode: 32, total length: 107, average length of prev 10: 558.6 \n",
      "\n",
      "Loss: [0.8096401]\n",
      "Current State: [10 10]\n",
      "episode: 34, total length: 493, average length of prev 10: 513.2 \n",
      "\n",
      "Loss: [3.3682954]\n",
      "Current State: [10 10]\n",
      "episode: 36, total length: 350, average length of prev 10: 427.2 \n",
      "\n",
      "Loss: [0.03473986]\n",
      "Current State: [10 10]\n",
      "episode: 38, total length: 807, average length of prev 10: 353.4 \n",
      "\n",
      "Loss: [10.3132715]\n",
      "Current State: [10 10]\n",
      "episode: 40, total length: 238, average length of prev 10: 413.9 \n",
      "\n",
      "Loss: [9.084367]\n",
      "Current State: [10 10]\n",
      "episode: 42, total length: 252, average length of prev 10: 435.5 \n",
      "\n",
      "Loss: [43.234917]\n",
      "Current State: [10 10]\n",
      "episode: 44, total length: 96, average length of prev 10: 390.3 \n",
      "\n",
      "Loss: [45.428806]\n",
      "Current State: [10 10]\n",
      "episode: 46, total length: 87, average length of prev 10: 387.3 \n",
      "\n",
      "Loss: [2.0781868]\n",
      "Current State: [10 10]\n",
      "episode: 48, total length: 393, average length of prev 10: 358.2 \n",
      "\n",
      "Loss: [0.00564104]\n",
      "Current State: [9 9]\n",
      "episode: 50, total length: 1000, average length of prev 10: 434.4 \n",
      "\n",
      "Loss: [28.319035]\n",
      "Current State: [10 10]\n",
      "episode: 52, total length: 132, average length of prev 10: 411.2 \n",
      "\n",
      "Loss: [6.9053063]\n",
      "Current State: [10 10]\n",
      "episode: 54, total length: 277, average length of prev 10: 442.3 \n",
      "\n",
      "Loss: [34.523518]\n",
      "Current State: [10 10]\n",
      "episode: 56, total length: 109, average length of prev 10: 499.4 \n",
      "\n",
      "Loss: [13.61896]\n",
      "Current State: [10 10]\n",
      "episode: 58, total length: 193, average length of prev 10: 464.7 \n",
      "\n",
      "Loss: [0.48073164]\n",
      "Current State: [10 10]\n",
      "episode: 60, total length: 521, average length of prev 10: 325.5 \n",
      "\n",
      "Loss: [8.901233]\n",
      "Current State: [10 10]\n",
      "episode: 62, total length: 241, average length of prev 10: 321.2 \n",
      "\n",
      "Loss: [7.046817]\n",
      "Current State: [10 10]\n",
      "episode: 64, total length: 251, average length of prev 10: 304.0 \n",
      "\n",
      "Loss: [0.69311154]\n",
      "Current State: [10 10]\n",
      "episode: 66, total length: 492, average length of prev 10: 258.6 \n",
      "\n",
      "Loss: [0.8764454]\n",
      "Current State: [10 10]\n",
      "episode: 68, total length: 466, average length of prev 10: 289.0 \n",
      "\n",
      "Loss: [21.345562]\n",
      "Current State: [10 10]\n",
      "episode: 70, total length: 139, average length of prev 10: 247.1 \n",
      "\n",
      "Loss: [11.330528]\n",
      "Current State: [10 10]\n",
      "episode: 72, total length: 203, average length of prev 10: 246.3 \n",
      "\n",
      "Loss: [0.00508054]\n",
      "Current State: [ 3 10]\n",
      "episode: 74, total length: 1000, average length of prev 10: 339.1 \n",
      "\n",
      "Loss: [13.020772]\n",
      "Current State: [10 10]\n",
      "episode: 76, total length: 176, average length of prev 10: 299.8 \n",
      "\n",
      "Loss: [0.00599127]\n",
      "Current State: [ 2 10]\n",
      "episode: 78, total length: 1000, average length of prev 10: 431.0 \n",
      "\n",
      "Loss: [0.00519043]\n",
      "Current State: [ 0 10]\n",
      "episode: 80, total length: 1000, average length of prev 10: 537.7 \n",
      "\n",
      "Loss: [0.00658614]\n",
      "Current State: [ 3 10]\n",
      "episode: 82, total length: 1000, average length of prev 10: 693.0 \n",
      "\n",
      "Loss: [0.00535518]\n",
      "Current State: [ 3 10]\n",
      "episode: 84, total length: 1000, average length of prev 10: 641.9 \n",
      "\n",
      "Loss: [0.0057239]\n",
      "Current State: [ 2 10]\n",
      "episode: 86, total length: 1000, average length of prev 10: 815.7 \n",
      "\n",
      "Loss: [0.00584216]\n",
      "Current State: [2 9]\n",
      "episode: 88, total length: 1000, average length of prev 10: 753.2 \n",
      "\n",
      "Loss: [0.0059387]\n",
      "Current State: [3 9]\n",
      "episode: 90, total length: 1000, average length of prev 10: 827.6 \n",
      "\n",
      "Loss: [0.00663227]\n",
      "Current State: [ 2 10]\n",
      "episode: 92, total length: 1000, average length of prev 10: 770.8 \n",
      "\n",
      "Loss: [0.00726215]\n",
      "Current State: [ 3 10]\n",
      "episode: 94, total length: 1000, average length of prev 10: 866.8 \n",
      "\n",
      "Loss: [0.00935822]\n",
      "Current State: [ 3 10]\n",
      "episode: 96, total length: 1000, average length of prev 10: 866.8 \n",
      "\n",
      "Loss: [12.413447]\n",
      "Current State: [10 10]\n",
      "episode: 98, total length: 273, average length of prev 10: 774.6 \n",
      "\n",
      "Loss: [0.01046028]\n",
      "Current State: [ 3 10]\n",
      "episode: 100, total length: 1000, average length of prev 10: 711.7 \n",
      "\n",
      "Loss: [0.05671285]\n",
      "Current State: [10 10]\n",
      "episode: 102, total length: 813, average length of prev 10: 706.1 \n",
      "\n",
      "Loss: [0.01063145]\n",
      "Current State: [ 3 10]\n",
      "episode: 104, total length: 1000, average length of prev 10: 706.1 \n",
      "\n",
      "Loss: [0.01778955]\n",
      "Current State: [10 10]\n",
      "episode: 106, total length: 954, average length of prev 10: 701.5 \n",
      "\n",
      "Loss: [65.4008]\n",
      "Current State: [10 10]\n",
      "episode: 108, total length: 138, average length of prev 10: 778.5 \n",
      "\n",
      "Loss: [0.01322071]\n",
      "Current State: [ 3 10]\n",
      "episode: 110, total length: 1000, average length of prev 10: 839.2 \n",
      "\n",
      "Loss: [0.01386724]\n",
      "Current State: [ 2 10]\n",
      "episode: 112, total length: 1000, average length of prev 10: 907.0 \n",
      "\n",
      "Loss: [0.01493771]\n",
      "Current State: [ 2 10]\n",
      "episode: 114, total length: 1000, average length of prev 10: 907.0 \n",
      "\n",
      "Loss: [0.01724978]\n",
      "Current State: [ 2 10]\n",
      "episode: 116, total length: 1000, average length of prev 10: 911.6 \n",
      "\n",
      "Loss: [0.02266778]\n",
      "Current State: [ 2 10]\n",
      "episode: 118, total length: 1000, average length of prev 10: 997.8 \n",
      "\n",
      "Loss: [0.03181365]\n",
      "Current State: [ 3 10]\n",
      "episode: 120, total length: 1000, average length of prev 10: 998.4 \n",
      "\n",
      "Loss: [0.16799995]\n",
      "Current State: [10 10]\n",
      "episode: 122, total length: 859, average length of prev 10: 984.3 \n",
      "\n",
      "Loss: [241.25024]\n",
      "Current State: [10 10]\n",
      "episode: 124, total length: 164, average length of prev 10: 900.7 \n",
      "\n",
      "Loss: [0.06023779]\n",
      "Current State: [ 3 10]\n",
      "episode: 126, total length: 1000, average length of prev 10: 900.7 \n",
      "\n",
      "Loss: [0.05607758]\n",
      "Current State: [ 3 10]\n",
      "episode: 128, total length: 1000, average length of prev 10: 807.3 \n",
      "\n",
      "Loss: [0.05611635]\n",
      "Current State: [ 2 10]\n",
      "episode: 130, total length: 1000, average length of prev 10: 808.9 \n",
      "\n",
      "Loss: [0.0566638]\n",
      "Current State: [ 2 10]\n",
      "episode: 132, total length: 1000, average length of prev 10: 823.0 \n",
      "\n",
      "Loss: [0.05765622]\n",
      "Current State: [ 3 10]\n",
      "episode: 134, total length: 1000, average length of prev 10: 906.6 \n",
      "\n",
      "Loss: [0.05986421]\n",
      "Current State: [ 3 10]\n",
      "episode: 136, total length: 1000, average length of prev 10: 906.6 \n",
      "\n",
      "Loss: [0.06547303]\n",
      "Current State: [ 2 10]\n",
      "episode: 138, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.07900503]\n",
      "Current State: [ 0 10]\n",
      "episode: 140, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.10234778]\n",
      "Current State: [ 2 10]\n",
      "episode: 142, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.13376626]\n",
      "Current State: [ 3 10]\n",
      "episode: 144, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.15996243]\n",
      "Current State: [ 3 10]\n",
      "episode: 146, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.18039513]\n",
      "Current State: [ 1 10]\n",
      "episode: 148, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.19412187]\n",
      "Current State: [ 0 10]\n",
      "episode: 150, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.20170105]\n",
      "Current State: [ 3 10]\n",
      "episode: 152, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.20466495]\n",
      "Current State: [ 0 10]\n",
      "episode: 154, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.20676044]\n",
      "Current State: [ 0 10]\n",
      "episode: 156, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.20828918]\n",
      "Current State: [ 0 10]\n",
      "episode: 158, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.20926717]\n",
      "Current State: [ 2 10]\n",
      "episode: 160, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.20926608]\n",
      "Current State: [ 3 10]\n",
      "episode: 162, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.20939903]\n",
      "Current State: [ 2 10]\n",
      "episode: 164, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.21084714]\n",
      "Current State: [ 1 10]\n",
      "episode: 166, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.21096641]\n",
      "Current State: [ 1 10]\n",
      "episode: 168, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.21205465]\n",
      "Current State: [ 1 10]\n",
      "episode: 170, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.2079263]\n",
      "Current State: [ 1 10]\n",
      "episode: 172, total length: 1000, average length of prev 10: 949.8 \n",
      "\n",
      "Loss: [0.20842591]\n",
      "Current State: [ 1 10]\n",
      "episode: 174, total length: 1000, average length of prev 10: 949.8 \n",
      "\n",
      "Loss: [0.21030718]\n",
      "Current State: [ 0 10]\n",
      "episode: 176, total length: 1000, average length of prev 10: 949.8 \n",
      "\n",
      "Loss: [0.21099149]\n",
      "Current State: [ 0 10]\n",
      "episode: 178, total length: 1000, average length of prev 10: 949.8 \n",
      "\n",
      "Loss: [0.22242564]\n",
      "Current State: [ 0 10]\n",
      "episode: 180, total length: 1000, average length of prev 10: 949.8 \n",
      "\n",
      "Loss: [0.21297523]\n",
      "Current State: [ 1 10]\n",
      "episode: 182, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.214165]\n",
      "Current State: [ 1 10]\n",
      "episode: 184, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.21390899]\n",
      "Current State: [ 0 10]\n",
      "episode: 186, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.22375987]\n",
      "Current State: [3 9]\n",
      "episode: 188, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.21855643]\n",
      "Current State: [ 3 10]\n",
      "episode: 190, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [475.13925]\n",
      "Current State: [10 10]\n",
      "episode: 192, total length: 225, average length of prev 10: 861.0 \n",
      "\n",
      "Loss: [382.15417]\n",
      "Current State: [10 10]\n",
      "episode: 194, total length: 244, average length of prev 10: 785.4 \n",
      "\n",
      "Loss: [507.8582]\n",
      "Current State: [10 10]\n",
      "episode: 196, total length: 212, average length of prev 10: 633.4 \n",
      "\n",
      "Loss: [103.78294]\n",
      "Current State: [10 10]\n",
      "episode: 198, total length: 368, average length of prev 10: 532.0 \n",
      "\n",
      "Loss: [231.98524]\n",
      "Current State: [10 10]\n",
      "episode: 200, total length: 287, average length of prev 10: 405.6 \n",
      "\n",
      "Loss: [1155.778]\n",
      "Current State: [10 10]\n",
      "episode: 202, total length: 123, average length of prev 10: 360.0 \n",
      "\n",
      "Loss: [1270.8386]\n",
      "Current State: [10 10]\n",
      "episode: 204, total length: 111, average length of prev 10: 252.9 \n",
      "\n",
      "Loss: [2327.3904]\n",
      "Current State: [10 10]\n",
      "episode: 206, total length: 49, average length of prev 10: 218.4 \n",
      "\n",
      "Loss: [1424.5605]\n",
      "Current State: [10 10]\n",
      "episode: 208, total length: 95, average length of prev 10: 133.1 \n",
      "\n",
      "Loss: [1922.5872]\n",
      "Current State: [10 10]\n",
      "episode: 210, total length: 69, average length of prev 10: 99.4 \n",
      "\n",
      "Loss: [2008.1433]\n",
      "Current State: [10 10]\n",
      "episode: 212, total length: 59, average length of prev 10: 119.3 \n",
      "\n",
      "Loss: [762.4021]\n",
      "Current State: [10 10]\n",
      "episode: 214, total length: 154, average length of prev 10: 159.9 \n",
      "\n",
      "Loss: [50.887817]\n",
      "Current State: [10 10]\n",
      "episode: 216, total length: 427, average length of prev 10: 289.1 \n",
      "\n",
      "Loss: [162.69168]\n",
      "Current State: [10 10]\n",
      "episode: 218, total length: 307, average length of prev 10: 406.5 \n",
      "\n",
      "Loss: [102.20041]\n",
      "Current State: [10 10]\n",
      "episode: 220, total length: 353, average length of prev 10: 460.8 \n",
      "\n",
      "Loss: [0.6666455]\n",
      "Current State: [10 10]\n",
      "episode: 222, total length: 852, average length of prev 10: 523.9 \n",
      "\n",
      "Loss: [0.16719782]\n",
      "Current State: [ 3 10]\n",
      "episode: 224, total length: 1000, average length of prev 10: 666.0 \n",
      "\n",
      "Loss: [0.16276616]\n",
      "Current State: [ 3 10]\n",
      "episode: 226, total length: 1000, average length of prev 10: 630.2 \n",
      "\n",
      "Loss: [6.8083963]\n",
      "Current State: [10 10]\n",
      "episode: 228, total length: 616, average length of prev 10: 573.3 \n",
      "\n",
      "Loss: [0.15985668]\n",
      "Current State: [3 9]\n",
      "episode: 230, total length: 1000, average length of prev 10: 679.1 \n",
      "\n",
      "Loss: [0.1599882]\n",
      "Current State: [ 3 10]\n",
      "episode: 232, total length: 1000, average length of prev 10: 780.7 \n",
      "\n",
      "Loss: [0.35678217]\n",
      "Current State: [10 10]\n",
      "episode: 234, total length: 911, average length of prev 10: 771.8 \n",
      "\n",
      "Loss: [0.17360987]\n",
      "Current State: [ 2 10]\n",
      "episode: 236, total length: 1000, average length of prev 10: 864.9 \n",
      "\n",
      "Loss: [787.23737]\n",
      "Current State: [10 10]\n",
      "episode: 238, total length: 148, average length of prev 10: 905.9 \n",
      "\n",
      "Loss: [40.84898]\n",
      "Current State: [10 10]\n",
      "episode: 240, total length: 438, average length of prev 10: 849.7 \n",
      "\n",
      "Loss: [0.15655702]\n",
      "Current State: [ 3 10]\n",
      "episode: 242, total length: 1000, average length of prev 10: 760.9 \n",
      "\n",
      "Loss: [0.2091937]\n",
      "Current State: [10 10]\n",
      "episode: 244, total length: 962, average length of prev 10: 766.0 \n",
      "\n",
      "Loss: [0.16237283]\n",
      "Current State: [4 4]\n",
      "episode: 246, total length: 1000, average length of prev 10: 766.0 \n",
      "\n",
      "Loss: [2464.922]\n",
      "Current State: [10 10]\n",
      "episode: 248, total length: 29, average length of prev 10: 708.9 \n",
      "\n",
      "Loss: [21.874702]\n",
      "Current State: [10 10]\n",
      "episode: 250, total length: 496, average length of prev 10: 714.7 \n",
      "\n",
      "Loss: [0.15273906]\n",
      "Current State: [ 3 10]\n",
      "episode: 252, total length: 1000, average length of prev 10: 803.5 \n",
      "\n",
      "Loss: [0.15845983]\n",
      "Current State: [4 4]\n",
      "episode: 254, total length: 1000, average length of prev 10: 805.4 \n",
      "\n",
      "Loss: [0.15359789]\n",
      "Current State: [ 3 10]\n",
      "episode: 256, total length: 1000, average length of prev 10: 805.4 \n",
      "\n",
      "Loss: [0.15300407]\n",
      "Current State: [ 1 10]\n",
      "episode: 258, total length: 1000, average length of prev 10: 887.5 \n",
      "\n",
      "Loss: [179.10385]\n",
      "Current State: [10 10]\n",
      "episode: 260, total length: 286, average length of prev 10: 829.4 \n",
      "\n",
      "Loss: [0.14963433]\n",
      "Current State: [ 3 10]\n",
      "episode: 262, total length: 1000, average length of prev 10: 829.4 \n",
      "\n",
      "Loss: [2414.434]\n",
      "Current State: [10 10]\n",
      "episode: 264, total length: 21, average length of prev 10: 660.5 \n",
      "\n",
      "Loss: [114.9247]\n",
      "Current State: [10 10]\n",
      "episode: 266, total length: 321, average length of prev 10: 592.6 \n",
      "\n",
      "Loss: [193.36008]\n",
      "Current State: [10 10]\n",
      "episode: 268, total length: 269, average length of prev 10: 579.7 \n",
      "\n",
      "Loss: [68.14408]\n",
      "Current State: [10 10]\n",
      "episode: 270, total length: 372, average length of prev 10: 621.7 \n",
      "\n",
      "Loss: [0.14609516]\n",
      "Current State: [ 2 10]\n",
      "episode: 272, total length: 1000, average length of prev 10: 621.7 \n",
      "\n",
      "Loss: [0.14254896]\n",
      "Current State: [3 9]\n",
      "episode: 274, total length: 1000, average length of prev 10: 792.5 \n",
      "\n",
      "Loss: [0.14409027]\n",
      "Current State: [ 3 10]\n",
      "episode: 276, total length: 1000, average length of prev 10: 860.4 \n",
      "\n",
      "Loss: [0.14819095]\n",
      "Current State: [ 3 10]\n",
      "episode: 278, total length: 1000, average length of prev 10: 933.5 \n",
      "\n",
      "Loss: [0.14568576]\n",
      "Current State: [10  4]\n",
      "episode: 280, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.15221082]\n",
      "Current State: [10  4]\n",
      "episode: 282, total length: 1000, average length of prev 10: 980.8 \n",
      "\n",
      "Loss: [0.16667446]\n",
      "Current State: [10  4]\n",
      "episode: 284, total length: 1000, average length of prev 10: 980.8 \n",
      "\n",
      "Loss: [0.17852716]\n",
      "Current State: [10  4]\n",
      "episode: 286, total length: 1000, average length of prev 10: 980.8 \n",
      "\n",
      "Loss: [0.18562394]\n",
      "Current State: [10  4]\n",
      "episode: 288, total length: 1000, average length of prev 10: 980.8 \n",
      "\n",
      "Loss: [0.19139233]\n",
      "Current State: [10  4]\n",
      "episode: 290, total length: 1000, average length of prev 10: 980.8 \n",
      "\n",
      "Loss: [0.19490981]\n",
      "Current State: [10  4]\n",
      "episode: 292, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.19807497]\n",
      "Current State: [10  4]\n",
      "episode: 294, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.19923095]\n",
      "Current State: [10  4]\n",
      "episode: 296, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [3184.548]\n",
      "Current State: [10 10]\n",
      "episode: 298, total length: 31, average length of prev 10: 903.1 \n",
      "\n",
      "Loss: [0.1910302]\n",
      "Current State: [10  4]\n",
      "episode: 300, total length: 1000, average length of prev 10: 903.1 \n",
      "\n",
      "Loss: [0.1913407]\n",
      "Current State: [10  4]\n",
      "episode: 302, total length: 1000, average length of prev 10: 903.1 \n",
      "\n",
      "Loss: [0.1937419]\n",
      "Current State: [10  4]\n",
      "episode: 304, total length: 1000, average length of prev 10: 903.1 \n",
      "\n",
      "Loss: [0.19230604]\n",
      "Current State: [10  3]\n",
      "episode: 306, total length: 1000, average length of prev 10: 903.1 \n",
      "\n",
      "Loss: [0.19737172]\n",
      "Current State: [10  4]\n",
      "episode: 308, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.19385402]\n",
      "Current State: [10  4]\n",
      "episode: 310, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.1957351]\n",
      "Current State: [10  4]\n",
      "episode: 312, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.19876534]\n",
      "Current State: [10  4]\n",
      "episode: 314, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.20373634]\n",
      "Current State: [10  4]\n",
      "episode: 316, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.20718215]\n",
      "Current State: [10  4]\n",
      "episode: 318, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.20921353]\n",
      "Current State: [10  4]\n",
      "episode: 320, total length: 1000, average length of prev 10: 1000.0 \n",
      "\n",
      "Loss: [0.20899014]\n",
      "Current State: [10  4]\n",
      "episode: 322, total length: 1000, average length of prev 10: 930.6 \n",
      "\n",
      "Loss: [0.20663461]\n",
      "Current State: [10  4]\n",
      "episode: 324, total length: 1000, average length of prev 10: 930.6 \n",
      "\n",
      "Loss: [0.20507485]\n",
      "Current State: [10  4]\n",
      "episode: 326, total length: 1000, average length of prev 10: 930.6 \n",
      "\n",
      "Loss: [0.20543696]\n",
      "Current State: [10  4]\n",
      "episode: 328, total length: 1000, average length of prev 10: 930.6 \n",
      "\n",
      "Loss: [0.20626687]\n",
      "Current State: [10  4]\n",
      "episode: 330, total length: 1000, average length of prev 10: 930.6 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[1;32m     18\u001b[0m     agent \u001b[39m=\u001b[39m SAC(four_rooms_env_jitter, state_height, hidden_size, output_size, learning_rate, num_episodes\u001b[39m=\u001b[39mmax_episodes, num_steps\u001b[39m=\u001b[39mnum_steps, gamma\u001b[39m=\u001b[39mgamma)\n\u001b[0;32m---> 19\u001b[0m     all_lengths, average_lengths, all_entropies, all_losses, episode_rewards \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     21\u001b[0m     many_run_average_lengths\u001b[39m.\u001b[39mappend(average_lengths)\n\u001b[1;32m     23\u001b[0m     \u001b[39m# rolling_avg_entropies = pd.Series(all_entropies).rolling(10).mean()\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[39m# many_run_entropies.append(rolling_avg_entropies)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 58\u001b[0m, in \u001b[0;36mSAC.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m ac_loss \u001b[39m=\u001b[39m actor_loss \u001b[39m+\u001b[39m critic_loss \u001b[39m+\u001b[39m \u001b[39m0.001\u001b[39m \u001b[39m*\u001b[39m entropy\n\u001b[1;32m     57\u001b[0m ac_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 58\u001b[0m ac_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     59\u001b[0m ac_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     60\u001b[0m \u001b[39m# all_entropies.append(entropy)\u001b[39;00m\n",
      "File \u001b[0;32m~/code/CS5180-project/venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/code/CS5180-project/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "These results are generated with a2c reward of 1 on episode completion and -0.5 on every step\n",
    "\"\"\"\n",
    "hidden_size = 64\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Constants\n",
    "gamma = 0.99\n",
    "num_steps = 1\n",
    "max_episodes = 1000\n",
    "state_height = 11\n",
    "\n",
    "# 22 inputs. 11 for the current state and 11 for the next state. To one hot encode our integer values.\n",
    "output_size = four_rooms_env_no_jitter.action_space.n\n",
    "many_run_lengths, many_run_average_lengths, many_run_entropies, many_run_losses, many_run_rewards = [], [], [], [], []\n",
    "\n",
    "for i in range(5):\n",
    "    agent = SAC(four_rooms_env_no_jitter, state_height, hidden_size, output_size, learning_rate, num_episodes=max_episodes, num_steps=num_steps, gamma=gamma)\n",
    "    all_lengths, average_lengths, all_entropies, all_losses, episode_rewards = agent.train()\n",
    "\n",
    "    many_run_average_lengths.append(average_lengths)\n",
    "\n",
    "    # rolling_avg_entropies = pd.Series(all_entropies).rolling(10).mean()\n",
    "    # many_run_entropies.append(rolling_avg_entropies)\n",
    "    \n",
    "    rolling_avg_losses = pd.Series(all_losses).rolling(10).mean()\n",
    "    many_run_losses.append(rolling_avg_losses)\n",
    "\n",
    "    many_run_rewards.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACEligibility(A2CTD0):\n",
    "\n",
    "    def __init__(self, env, obs_size, hidden_size, output_size, learning_rate, num_episodes, num_steps, gamma, lambda_) -> None:\n",
    "        super().__init__(env, obs_size, hidden_size, output_size, learning_rate, num_episodes, num_steps, gamma)\n",
    "        self.lambda_ = lambda_\n",
    "        # self.model = ActorCriticSingleAgent(obs_size, hidden_size, output_size)\n",
    "    \n",
    "    def train(self):\n",
    "        # Use Adam optimizer for the actor-critic because it should converge faster than SGD and generalization may not be super important\n",
    "        ac_optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        # all episode length\n",
    "        all_lengths = []\n",
    "        all_losses = []\n",
    "        average_lengths = []\n",
    "        episode_rewards = []\n",
    "        for episode in range(self.num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            i = 1\n",
    "            z_critics = torch.zeros(1, 1)\n",
    "            z_actors = torch.zeros(1, 1)\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                state_tensor = self._obs_to_tensor(state)\n",
    "                critic_current_value, actor_act_pred = self.model.forward(state_tensor.float())\n",
    "                detached_act_pred = actor_act_pred.detach().numpy()\n",
    "                action = self._get_action(detached_act_pred)\n",
    "                new_state, reward, done, _, _ = self.env.step(action)\n",
    "                if not done:\n",
    "                    new_state_tensor = self._obs_to_tensor(new_state)\n",
    "                    next_state_critic, _ = self.model.forward(new_state_tensor.float())\n",
    "                else:\n",
    "                    all_lengths.append(steps)\n",
    "                    average_lengths.append(np.mean(all_lengths[-10:]))\n",
    "                    next_state_critic = torch.tensor(0)\n",
    "\n",
    "                z_critic = self.lambda_ * self.gamma * z_critics.detach() + critic_current_value\n",
    "                z_actors = self.lambda_ * self.gamma * z_actors.detach() + i * -torch.distributions.Categorical(actor_act_pred).log_prob(torch.tensor(action))\n",
    "                td_error = reward + self.gamma * next_state_critic - critic_current_value\n",
    "\n",
    "                td_error_no_grad = td_error.detach()\n",
    "\n",
    "                actor_loss = td_error_no_grad * z_actors\n",
    "                critic_loss = -z_critic * td_error\n",
    "                i = self.gamma * i\n",
    "                steps += 1\n",
    "                state = new_state\n",
    "                ac_loss = actor_loss + critic_loss\n",
    "                ac_optimizer.zero_grad()\n",
    "                ac_loss.backward()\n",
    "                ac_optimizer.step()\n",
    "                all_losses.append(ac_loss.detach().numpy())\n",
    "\n",
    "                if done:\n",
    "                    self._show_episode_results(episode, steps, state, all_losses, average_lengths)\n",
    "                    \n",
    "            \n",
    "        return all_lengths, average_lengths, all_entropies, all_losses, episode_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: [[3.530126]]\n",
      "Current State: [10 10]\n",
      "episode: 0, total length: 246, average length of prev 10: 245.0 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "These results are generated with a2c reward of 1 on episode completion and -0.5 on every step\n",
    "\"\"\"\n",
    "hidden_size = 64\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Constants\n",
    "gamma = 0.99\n",
    "num_steps = 1\n",
    "max_episodes = 1000\n",
    "state_height = 11\n",
    "\n",
    "# 22 inputs. 11 for the current state and 11 for the next state. To one hot encode our integer values.\n",
    "output_size = four_rooms_env_no_jitter.action_space.n\n",
    "many_run_lengths, many_run_average_lengths, many_run_entropies, many_run_losses, many_run_rewards = [], [], [], [], []\n",
    "\n",
    "for i in range(5):\n",
    "    agent = ACEligibility(four_rooms_env_no_jitter, state_height, hidden_size, output_size, learning_rate, num_episodes=max_episodes, num_steps=num_steps, gamma=gamma, lambda_=0.9)\n",
    "    all_lengths, average_lengths, all_entropies, all_losses, episode_rewards = agent.train()\n",
    "\n",
    "    many_run_average_lengths.append(average_lengths)\n",
    "\n",
    "    # rolling_avg_entropies = pd.Series(all_entropies).rolling(10).mean()\n",
    "    # many_run_entropies.append(rolling_avg_entropies)\n",
    "    \n",
    "    rolling_avg_losses = pd.Series(all_losses).rolling(10).mean()\n",
    "    many_run_losses.append(rolling_avg_losses)\n",
    "\n",
    "    many_run_rewards.append(episode_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "606dcba7ea3d5e6ab6b9b5ab16aaa6ef92f938b8f3abc4af7f21838acfce664b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
