{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.22.1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pettingzoo.mpe import simple_spread_v2\n",
    "\n",
    "env = simple_spread_v2.parallel_env(N=3, local_ratio=0.5, max_cycles=25, continuous_actions=False)\n",
    "\n",
    "import pettingzoo\n",
    "pettingzoo.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_0': array([ 0.31500483,  0.59926844, -1.0832875 , -0.06565291,  0.28186268,\n",
      "        0.11397307,  0.16470452, -0.17338154,  0.99939805,  0.0890506 ,\n",
      "        0.01904923,  0.2580465 ,  0.91106004, -0.7305967 ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ], dtype=float32), 'agent_1': array([-0.7975387 , -0.51615536, -1.0642382 ,  0.19239359,  0.26281345,\n",
      "       -0.14407343,  0.14565529, -0.43142805,  0.9803488 , -0.1689959 ,\n",
      "       -0.01904923, -0.2580465 ,  0.8920108 , -0.9886432 ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ], dtype=float32), 'agent_2': array([ 0.26891196, -0.30733103, -0.17222741, -0.7962496 , -0.62919736,\n",
      "        0.8445698 , -0.74635553,  0.5572152 ,  0.088338  ,  0.8196473 ,\n",
      "       -0.91106004,  0.7305967 , -0.8920108 ,  0.9886432 ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "while env.agents:\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}  # this is where you would insert your policy\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a single agent actor-critic implementation.\n",
    "It is specifically designed to work with the four rooms environment, but it should work with any gym environment.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def obs_to_tensor(obs: np.array) -> torch.Tensor:\n",
    "    # initialize an empty np array\n",
    "    obs_tensor = np.array([])\n",
    "    for _, value in obs.items():\n",
    "        obs_tensor = np.concatenate((obs_tensor, value))\n",
    "    return torch.Tensor(obs_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env\u001b[39m.\u001b[39;49mobservation_space\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticSingleAgent(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple, but flexible, implementation of the actor-critic algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
    "        super(ActorCriticSingleAgent, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        # estimate the value function\n",
    "        self.critic_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.critic_linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # estimate the policy distribution\n",
    "        self.actor_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.actor_linear2 = nn.Linear(hidden_size, num_actions)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Do inference to calculate the action probabilities and the state value.\n",
    "        \"\"\"\n",
    "        value = F.relu(self.critic_linear1(state))\n",
    "        value = self.critic_linear2(value)\n",
    "        \n",
    "        policy_dist = F.relu(self.actor_linear1(state))\n",
    "        # softmax effectively generates a probability for each of our output options\n",
    "        policy_dist = F.softmax(self.actor_linear2(policy_dist))\n",
    "\n",
    "        return value, policy_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This doesn't quite use entropy correctly and it doesn't bootstrap correctly.\n",
    "BUT. it is a start.\n",
    "\"\"\"\n",
    "def a2c(env, hidden_size, learning_rate, num_episodes, num_steps, gamma=0.99, num_classes=11):\n",
    "    num_inputs = env.observation_space.shape[0] * num_classes\n",
    "    num_outputs = env.action_space.n\n",
    "    \n",
    "    actor_critic = ActorCriticSingleAgent(num_inputs, num_outputs, hidden_size)\n",
    "    # Use Adam optimizer for the actor-critic because it should converge faster than SGD and generalization may not be super important\n",
    "    ac_optimizer = optim.Adam(actor_critic.parameters(), lr=learning_rate)\n",
    "\n",
    "    # all episode length\n",
    "    all_lengths = []\n",
    "    # average episode length\n",
    "    average_lengths = []\n",
    "    # all episode rewards\n",
    "    all_rewards = []\n",
    "    \n",
    "    entropy_term = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "\n",
    "        state = env.reset()\n",
    "        for steps in range(num_steps):\n",
    "            # state_tensor = torch.flatten(F.one_hot(torch.tensor(state, dtype=torch.int64), num_classes=num_classes))\n",
    "            value, policy_dist = actor_critic.forward(state_tensor.float())\n",
    "            # drop the tensor dimension and computational graph info\n",
    "            value = value.detach().numpy()\n",
    "            dist = policy_dist.detach().numpy() \n",
    "\n",
    "            # Sample an action according the probs the network just output.\n",
    "            action = np.random.choice(num_outputs, p=np.squeeze(dist))\n",
    "            # Calculate the log probability of the action we've taken\n",
    "            log_prob = torch.log(policy_dist.squeeze(0)[action])\n",
    "            # \n",
    "            entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "            new_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            log_probs.append(log_prob)\n",
    "            entropy_term += entropy\n",
    "            state = new_state\n",
    "            \n",
    "            if done or steps == num_steps-1:\n",
    "                # state_tensor = torch.flatten(F.one_hot(torch.tensor(new_state, dtype=torch.int64), num_classes=num_classes))\n",
    "                Qval, _ = actor_critic.forward(state_tensor.float())\n",
    "                Qval = Qval.detach().numpy()\n",
    "                all_rewards.append(np.sum(rewards))\n",
    "                all_lengths.append(steps)\n",
    "                average_lengths.append(np.mean(all_lengths[-10:]))\n",
    "                if episode % 10 == 0:\n",
    "                    # Where total length is the number of steps taken in the episode and average length is average steps in all episodes seen \n",
    "                    sys.stdout.write(\"episode: {}, reward: {}, total length: {}, average length: {} \\n\".format(episode, np.sum(rewards), steps, average_lengths[-1]))\n",
    "                break\n",
    "        \n",
    "        # compute Q values\n",
    "        # These are the rewards plus discounted state values to calculate the advantage\n",
    "        Qvals = np.zeros_like(values)\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            Qval = rewards[t] + gamma * Qval\n",
    "            Qvals[t] = Qval\n",
    "  \n",
    "        #update actor critic\n",
    "        values = torch.FloatTensor(values)\n",
    "        Qvals = torch.FloatTensor(Qvals)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        \n",
    "        advantage = Qvals - values\n",
    "        actor_loss = (-log_probs * advantage).mean()\n",
    "        critic_loss = 0.5 * advantage.pow(2).mean()\n",
    "        ac_loss = actor_loss + critic_loss + 0.001 * entropy_term\n",
    "\n",
    "        ac_optimizer.zero_grad()\n",
    "        ac_loss.backward()\n",
    "        ac_optimizer.step()\n",
    "\n",
    "    return all_rewards, all_lengths, average_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C:\n",
    "    def __init__(self, env, obs_size, hidden_size, output_size, obs_to_tensor_fn, learning_rate, num_episodes, num_steps, gamma) -> None:\n",
    "        self.env = env\n",
    "        self.obs_size = obs_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.obs_to_tensor_fn = obs_to_tensor_fn\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_steps = num_steps\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.model = ActorCriticSingleAgent(self.obs_size, self.output_size, self.hidden_size, self.learning_rate)\n",
    "    \n",
    "    def _get_action_tensors(value, distance):\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        # Use Adam optimizer for the actor-critic because it should converge faster than SGD and generalization may not be super important\n",
    "        ac_optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        # all episode length\n",
    "        all_lengths = []\n",
    "        # average episode length\n",
    "        average_lengths = []\n",
    "        # all episode rewards\n",
    "        all_rewards = []\n",
    "        \n",
    "        entropy_term = 0\n",
    "\n",
    "        for episode in range(self.num_episodes):\n",
    "            log_probs = []\n",
    "            values = []\n",
    "            rewards = []\n",
    "\n",
    "            state = env.reset()\n",
    "            for steps in range(self.num_steps):\n",
    "                state_tensor = self.obs_to_tensor_fn(state)\n",
    "                # state_tensor = torch.flatten(F.one_hot(torch.tensor(state, dtype=torch.int64), num_classes=num_classes))\n",
    "                value, policy_dist = self.model.forward(state_tensor.float())\n",
    "                # drop the tensor dimension and computational graph info\n",
    "                value = value.detach().numpy()\n",
    "                dist = policy_dist.detach().numpy() \n",
    "\n",
    "                # TODO divide the action space for each agent up into sections.\n",
    "                # select each action accordingly.\n",
    "                # TODO use the parallel API in a separate function to apply these actions\n",
    "                actions = {agent: env.action_space(agent).sample() for agent in env.agents}  # this is where you would insert your policy\n",
    "                observations, rewards, terminations, truncations, infos = env.step(actions) \n",
    "                \n",
    "                # Sample an action according the probs the network just output.\n",
    "                action = np.random.choice(self.output_size, p=np.squeeze(dist))\n",
    "\n",
    "                # Calculate the log probability of the action we've taken\n",
    "                log_prob = torch.log(policy_dist.squeeze(0)[action])\n",
    "                # \n",
    "                entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "                new_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "                rewards.append(reward)\n",
    "                values.append(value)\n",
    "                log_probs.append(log_prob)\n",
    "                entropy_term += entropy\n",
    "                state = new_state\n",
    "                \n",
    "                if done or steps == self.num_steps - 1:\n",
    "                    # state_tensor = torch.flatten(F.one_hot(torch.tensor(new_state, dtype=torch.int64), num_classes=num_classes))\n",
    "                    Qval, _ = self.model.forward(state_tensor.float())\n",
    "                    Qval = Qval.detach().numpy()\n",
    "                    all_rewards.append(np.sum(rewards))\n",
    "                    all_lengths.append(steps)\n",
    "                    average_lengths.append(np.mean(all_lengths[-10:]))\n",
    "                    if episode % 10 == 0:\n",
    "                        # Where total length is the number of steps taken in the episode and average length is average steps in all episodes seen \n",
    "                        sys.stdout.write(\"episode: {}, reward: {}, total length: {}, average length: {} \\n\".format(episode, np.sum(rewards), steps, average_lengths[-1]))\n",
    "                    break\n",
    "            \n",
    "            # compute Q values\n",
    "            # These are the rewards plus discounted state values to calculate the advantage\n",
    "            Qvals = np.zeros_like(values)\n",
    "            for t in reversed(range(len(rewards))):\n",
    "                Qval = rewards[t] + self.gamma * Qval\n",
    "                Qvals[t] = Qval\n",
    "    \n",
    "            #update actor critic\n",
    "            values = torch.FloatTensor(values)\n",
    "            Qvals = torch.FloatTensor(Qvals)\n",
    "            log_probs = torch.stack(log_probs)\n",
    "            \n",
    "            advantage = Qvals - values\n",
    "            actor_loss = (-log_probs * advantage).mean()\n",
    "            critic_loss = 0.5 * advantage.pow(2).mean()\n",
    "            ac_loss = actor_loss + critic_loss + 0.001 * entropy_term\n",
    "\n",
    "            ac_optimizer.zero_grad()\n",
    "            ac_loss.backward()\n",
    "            ac_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115197/1729106046.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  policy_dist = F.softmax(self.actor_linear2(policy_dist))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [50], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m output_size \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space(\u001b[39mlist\u001b[39m(obs_dict\u001b[39m.\u001b[39mkeys())[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mn \u001b[39m*\u001b[39m env\u001b[39m.\u001b[39mnum_agents\n\u001b[1;32m     12\u001b[0m ac_class \u001b[39m=\u001b[39m A2C(env, obs_size, hidden_size, output_size, obs_to_tensor, learning_rate, num_episodes\u001b[39m=\u001b[39mmax_episodes, num_steps\u001b[39m=\u001b[39mnum_steps, gamma\u001b[39m=\u001b[39mgamma)\n\u001b[0;32m---> 13\u001b[0m all_rewards, all_lengths, average_lengths \u001b[39m=\u001b[39m ac_class\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[0;32mIn [49], line 47\u001b[0m, in \u001b[0;36mA2C.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m# \u001b[39;00m\n\u001b[1;32m     46\u001b[0m entropy \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mmean(dist) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlog(dist))\n\u001b[0;32m---> 47\u001b[0m new_state, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     49\u001b[0m rewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     50\u001b[0m values\u001b[39m.\u001b[39mappend(value)\n",
      "File \u001b[0;32m~/classes/RL/project/CS5180-project/venv/lib/python3.10/site-packages/pettingzoo/utils/conversions.py:157\u001b[0m, in \u001b[0;36maec_to_parallel_wrapper.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    154\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected agent \u001b[39m\u001b[39m{\u001b[39;00magent\u001b[39m}\u001b[39;00m\u001b[39m got agent \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maec_env\u001b[39m.\u001b[39magent_selection\u001b[39m}\u001b[39;00m\u001b[39m, Parallel environment wrapper expects agents to step in a cycle.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         )\n\u001b[1;32m    156\u001b[0m obs, rew, termination, truncation, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maec_env\u001b[39m.\u001b[39mlast()\n\u001b[0;32m--> 157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maec_env\u001b[39m.\u001b[39mstep(actions[agent])\n\u001b[1;32m    158\u001b[0m \u001b[39mfor\u001b[39;00m agent \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maec_env\u001b[39m.\u001b[39magents:\n\u001b[1;32m    159\u001b[0m     rewards[agent] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maec_env\u001b[39m.\u001b[39mrewards[agent]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "hidden_size = 64\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Constants\n",
    "gamma = 0.99\n",
    "max_episodes = 5000\n",
    "num_steps = 10_000\n",
    "\n",
    "obs_dict = env.reset()\n",
    "obs_size = obs_to_tensor(obs_dict).shape[0]\n",
    "output_size = env.action_space(list(obs_dict.keys())[0]).n * env.num_agents\n",
    "ac_class = A2C(env, obs_size, hidden_size, output_size, obs_to_tensor, learning_rate, num_episodes=max_episodes, num_steps=num_steps, gamma=gamma)\n",
    "all_rewards, all_lengths, average_lengths = ac_class.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e95bd2e9b31e81c6acdd4fdb88c8368cef1b094bdbe6f4c46c83eec4428ed37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
