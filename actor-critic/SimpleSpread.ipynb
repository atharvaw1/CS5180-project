{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/douglas/classes/RL/project/CS5180-project/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.mpe import simple_spread_v2\n",
    "\n",
    "env = simple_spread_v2.parallel_env(N=3, local_ratio=0.5, max_cycles=25, continuous_actions=False)\n",
    "\n",
    "import pettingzoo\n",
    "pettingzoo.__version__\n",
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_0': array([ 0.31500483,  0.59926844, -1.0832875 , -0.06565291,  0.28186268,\n",
      "        0.11397307,  0.16470452, -0.17338154,  0.99939805,  0.0890506 ,\n",
      "        0.01904923,  0.2580465 ,  0.91106004, -0.7305967 ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ], dtype=float32), 'agent_1': array([-0.7975387 , -0.51615536, -1.0642382 ,  0.19239359,  0.26281345,\n",
      "       -0.14407343,  0.14565529, -0.43142805,  0.9803488 , -0.1689959 ,\n",
      "       -0.01904923, -0.2580465 ,  0.8920108 , -0.9886432 ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ], dtype=float32), 'agent_2': array([ 0.26891196, -0.30733103, -0.17222741, -0.7962496 , -0.62919736,\n",
      "        0.8445698 , -0.74635553,  0.5572152 ,  0.088338  ,  0.8196473 ,\n",
      "       -0.91106004,  0.7305967 , -0.8920108 ,  0.9886432 ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# env.reset()\n",
    "\n",
    "# while env.agents:\n",
    "#     actions = {agent: env.action_space(agent).sample() for agent in env.agents}  # this is where you would insert your policy\n",
    "#     observations, rewards, terminations, truncations, infos = env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a single agent actor-critic implementation.\n",
    "It is specifically designed to work with the four rooms environment, but it should work with any gym environment.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def obs_to_tensor(obs: np.array) -> torch.Tensor:\n",
    "    # initialize an empty np array\n",
    "    obs_tensor = np.array([])\n",
    "    for _, value in obs.items():\n",
    "        obs_tensor = np.concatenate((obs_tensor, value))\n",
    "    return torch.Tensor(obs_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env\u001b[39m.\u001b[39;49mobservation_space\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticCentralizedMultiAgent(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple, but flexible, implementation of the actor-critic algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, num_actions, actions_per_agent, hidden_size, learning_rate=3e-4):\n",
    "        super(ActorCriticCentralizedMultiAgent, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.actions_per_agent = actions_per_agent\n",
    "        # estimate the value function\n",
    "        self.critic_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.critic_linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # estimate the policy distribution\n",
    "        self.actor_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.actor_linear2 = nn.Linear(hidden_size, num_actions)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Do inference to calculate the action probabilities and the state value.\n",
    "        \"\"\"\n",
    "        value = F.relu(self.critic_linear1(state))\n",
    "        value = self.critic_linear2(value)\n",
    "        \n",
    "        policy_dist = F.relu(self.actor_linear1(state))\n",
    "        # softmax effectively generates a probability for each of our output options\n",
    "        policy_dist = self.actor_linear2(policy_dist)\n",
    "        # reshape the policy dist for each actor\n",
    "        policy_dist = policy_dist.view(-1, self.actions_per_agent, self.num_actions)\n",
    "\n",
    "        policy_dist = F.softmax(policy_dist, 1)\n",
    "\n",
    "        return value, policy_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CCentralizedMultiAgent:\n",
    "    def __init__(self, env, obs_size, hidden_size, output_size, actions_per_agent, learning_rate, num_episodes, num_steps, gamma) -> None:\n",
    "        self.env = env\n",
    "        self.obs_size = obs_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_steps = num_steps\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.model = ActorCriticCentralizedMultiAgent(self.obs_size, self.output_size, actions_per_agent, self.hidden_size, self.learning_rate)\n",
    "    \n",
    "    def _get_actions(self, actor_act_pred, num_agents):\n",
    "        # Sample an action according the probs the network just output.\n",
    "        start = 0\n",
    "        actions = {}\n",
    "        actions = {agent: env.action_space(agent).sample() for agent in env.agents} \n",
    "        for agent in env.agents:\n",
    "            action_space_size = env.action_space(agent).n\n",
    "            action = np.random.choice(action_space_size, p=np.squeeze(actor_act_pred[start: start + action_space_size]))\n",
    "            start += action_space_size\n",
    "            actions[agent] = env.action_space(agent)[action]\n",
    "        return actions\n",
    "\n",
    "    def _obs_to_tensor(self, obs: np.array) -> torch.Tensor:\n",
    "        # initialize an empty np array\n",
    "        obs_tensor = np.array([])\n",
    "        for _, value in obs.items():\n",
    "            obs_tensor = np.concatenate((obs_tensor, value))\n",
    "        return torch.Tensor(obs_tensor)\n",
    "\n",
    "    def train(self):\n",
    "        # Use Adam optimizer for the actor-critic because it should converge faster than SGD and generalization may not be super important\n",
    "        ac_optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        # all episode length\n",
    "        all_lengths = []\n",
    "        # average episode length\n",
    "        average_lengths = []\n",
    "        all_entropies = []\n",
    "        all_losses = []\n",
    "        \n",
    "        entropy_term = 0\n",
    "\n",
    "        for episode in range(self.num_episodes):\n",
    "            log_probs = []\n",
    "            critic_td_errors = []\n",
    "            rewards = []\n",
    "\n",
    "            state = self.env.reset()\n",
    "            # enable n step actor critic. \n",
    "            for steps in range(self.num_steps):\n",
    "                state_tensor = self._obs_to_tensor(state)\n",
    "                critic_td_error, actor_act_pred = self.model.forward(state_tensor.float())\n",
    "                # drop the tensor dimension and computational graph info\n",
    "                critic_td_error = critic_td_error.detach().numpy()\n",
    "                detached_act_pred = actor_act_pred.detach().numpy()\n",
    "                actions = self._get_actions(detached_act_pred)\n",
    "\n",
    "                # Calculate the log probability of the actions we've taken\n",
    "                log_prob = torch.log(actor_act_pred.squeeze(0)[actions])\n",
    "\n",
    "                # Calculate the entropy/ uncertainty of the policy term. This is used to encourage exploration\n",
    "                entropy = -np.sum(np.mean(detached_act_pred) * np.log(detached_act_pred))\n",
    "                new_state, rewards, done, _, _ = self.env.step(actions)\n",
    "\n",
    "                rewards.append(np.mean(rewards))\n",
    "                critic_td_errors.append(critic_td_error)\n",
    "                log_probs.append(log_prob)\n",
    "                entropy_term += entropy\n",
    "                state = new_state\n",
    "                \n",
    "                if done or steps == self.num_steps - 1:\n",
    "                    state_tensor = self._obs_to_tensor(state)\n",
    "                    Qval, _ = self.model.forward(state_tensor.float())\n",
    "                    Qval = Qval.detach().numpy()\n",
    "                    all_lengths.append(steps)\n",
    "                    # use numpy to get the standard deviation of all lengths\n",
    "\n",
    "                    \n",
    "                    average_lengths.append(np.mean(all_lengths[-10:]))\n",
    "                    if episode % 30 == 0:\n",
    "                        # Where total length is the number of steps taken in the episode and average length is average steps in all episodes seen \n",
    "                        sys.stdout.write(\"episode: {}, total length: {}, average length of prev 10: {} \\n\".format(episode, steps, average_lengths[-1]))\n",
    "                    break\n",
    "            \n",
    "            # compute Q values\n",
    "            # These are the rewards plus discounted state values to calculate the advantage\n",
    "            Qvals = np.zeros_like(critic_td_errors)\n",
    "            for t in reversed(range(len(rewards))):\n",
    "                Qval = rewards[t] + self.gamma * Qval\n",
    "                Qvals[t] = Qval\n",
    "    \n",
    "            # update actor critic\n",
    "            critic_td_errors = torch.FloatTensor(np.array(critic_td_errors))\n",
    "            Qvals = torch.FloatTensor(Qvals)\n",
    "            log_probs = torch.stack(log_probs)\n",
    "            \n",
    "            advantage = Qvals - critic_td_errors\n",
    "            actor_loss = (-log_probs * advantage).mean()\n",
    "            critic_loss = 0.5 * advantage.pow(2).mean()\n",
    "            ac_loss = actor_loss + critic_loss + 0.001 * entropy_term\n",
    "\n",
    "            ac_optimizer.zero_grad()\n",
    "            ac_loss.backward()\n",
    "            ac_optimizer.step()\n",
    "            all_entropies.append(entropy_term)\n",
    "            all_losses.append(ac_loss.detach().numpy())\n",
    "        return all_lengths, average_lengths, all_entropies, all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "view(): argument 'size' must be tuple of SymInts, but found element of type list at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m     actions_per_agent\u001b[39m.\u001b[39mappend(env\u001b[39m.\u001b[39maction_space(\u001b[39mlist\u001b[39m(obs_dict\u001b[39m.\u001b[39mkeys())[i])\u001b[39m.\u001b[39mn)\n\u001b[1;32m     18\u001b[0m agent \u001b[39m=\u001b[39m A2CCentralizedMultiAgent(env, obs_size, hidden_size, output_size, actions_per_agent, learning_rate, num_episodes\u001b[39m=\u001b[39mmax_episodes, num_steps\u001b[39m=\u001b[39mnum_steps, gamma\u001b[39m=\u001b[39mgamma)\n\u001b[0;32m---> 20\u001b[0m all_lengths, average_lengths, all_entropies, all_losses \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[0;32mIn [18], line 54\u001b[0m, in \u001b[0;36mA2CCentralizedMultiAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m steps \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_steps):\n\u001b[1;32m     53\u001b[0m     state_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obs_to_tensor(state)\n\u001b[0;32m---> 54\u001b[0m     critic_td_error, actor_act_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward(state_tensor\u001b[39m.\u001b[39;49mfloat())\n\u001b[1;32m     55\u001b[0m     \u001b[39m# drop the tensor dimension and computational graph info\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     critic_td_error \u001b[39m=\u001b[39m critic_td_error\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn [4], line 29\u001b[0m, in \u001b[0;36mActorCriticCentralizedMultiAgent.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     27\u001b[0m policy_dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor_linear2(policy_dist)\n\u001b[1;32m     28\u001b[0m \u001b[39m# reshape the policy dist for each actor\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m policy_dist \u001b[39m=\u001b[39m policy_dist\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions_per_agent, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_actions)\n\u001b[1;32m     31\u001b[0m policy_dist \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(policy_dist, \u001b[39m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[39mreturn\u001b[39;00m value, policy_dist\n",
      "\u001b[0;31mTypeError\u001b[0m: view(): argument 'size' must be tuple of SymInts, but found element of type list at pos 2"
     ]
    }
   ],
   "source": [
    "hidden_size = 64\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Constants\n",
    "gamma = 0.99\n",
    "max_episodes = 5000\n",
    "num_steps = 10_000\n",
    "\n",
    "obs_dict = env.reset()\n",
    "obs_size = obs_to_tensor(obs_dict).shape[0]\n",
    "output_size = 0\n",
    "actions_per_agent = []\n",
    "\n",
    "for i in range(len(env.agents)):\n",
    "    output_size += env.action_space(env.agents[i]).n\n",
    "    actions_per_agent.append(env.action_space(list(obs_dict.keys())[i]).n)\n",
    "\n",
    "agent = A2CCentralizedMultiAgent(env, obs_size, hidden_size, output_size, actions_per_agent, learning_rate, num_episodes=max_episodes, num_steps=num_steps, gamma=gamma)\n",
    "\n",
    "all_lengths, average_lengths, all_entropies, all_losses = agent.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e95bd2e9b31e81c6acdd4fdb88c8368cef1b094bdbe6f4c46c83eec4428ed37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
