{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_spread_v2\n",
    "\n",
    "env = simple_spread_v2.parallel_env(N=3, local_ratio=0.5, max_cycles=500, continuous_actions=False)\n",
    "\n",
    "import pettingzoo\n",
    "pettingzoo.__version__\n",
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "# from tqdm import tqdm\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CTD0:\n",
    "    def __init__(self, env, obs_size, hidden_size, output_size, learning_rate, num_episodes, num_steps, gamma) -> None:\n",
    "        self.env = env\n",
    "        self.obs_size = obs_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_steps = num_steps\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # self.model = ActorCriticSingleAgent(self.obs_size * self.env.observation_space.shape[0] + 1, self.output_size, self.hidden_size, self.learning_rate)\n",
    "    \n",
    "    def _get_action(self, actor_act_pred):\n",
    "        # Sample an action according the probs the network just output.\n",
    "        action = np.random.choice(self.output_size, p=np.squeeze(actor_act_pred))\n",
    "        return action\n",
    "\n",
    "    def _obs_to_tensor(self, obs):\n",
    "        state_tensor = torch.flatten(F.one_hot(torch.tensor(obs, dtype=torch.int64), num_classes=self.obs_size))\n",
    "        # add a bias bit with a value of 1 in front of the one hot vector\n",
    "        state_tensor = torch.cat((torch.tensor([1.0]), state_tensor))\n",
    "        return state_tensor\n",
    "\n",
    "    def _show_episode_results(self, episode, steps, state, all_losses, average_lengths, frequency):\n",
    "        if episode % frequency == 0:\n",
    "            # make an array of the value of each state currently\n",
    "            # make an empty array for the values of each state of shape 10x10\n",
    "            state_values = np.zeros((10, 10))\n",
    "            for i in range(1, 11):\n",
    "                for j in range(1, 11):\n",
    "                    state_tensor = self._obs_to_tensor([i, j])\n",
    "                    value, _ = self.model.forward(state_tensor.float())\n",
    "                    state_values[i-1][j-1] = value.detach().numpy()\n",
    "            # show the value of each state with matplot lib\n",
    "            # plt.imshow(state_values)\n",
    "            # plt.show()\n",
    "                # sys.stdout.write(\"\\nState: \" + str(i) + \" Value: \" + str(state_values))\n",
    "            sys.stdout.write(\"\\nLoss: \" + str(all_losses[-1]) \n",
    "                # + \"\\nCurrent State: \" + str(state)\n",
    "                )\n",
    "            # Where total length is the number of steps taken in the episode and average length is average steps in all episodes seen \n",
    "            sys.stdout.write(\"\\nepisode: {}, total length: {}, average length of prev 10: {} \\n\".format(episode, steps, average_lengths[-1]))\n",
    "\n",
    "    def train(self):\n",
    "        # Use Adam optimizer for the actor-critic because it should converge faster than SGD and generalization may not be super important\n",
    "        ac_optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        # all episode length\n",
    "        all_lengths = []\n",
    "        episode_rewards = []\n",
    "        # average episode length\n",
    "        average_lengths = []\n",
    "        all_entropies = []\n",
    "        all_losses = []\n",
    "\n",
    "        for episode in range(self.num_episodes):\n",
    "            rewards = []\n",
    "            done = False\n",
    "            i = 1\n",
    "\n",
    "            state = self.env.reset()\n",
    "            steps = 0\n",
    "            # enable n step actor critic. \n",
    "            while not done:\n",
    "                state_tensor = self._obs_to_tensor(state)\n",
    "                critic_td_error, actor_act_pred = self.model.forward(state_tensor.float())\n",
    "                # drop the tensor dimension and computational graph info\n",
    "                detached_act_pred = actor_act_pred.detach().numpy()\n",
    "                action = self._get_action(detached_act_pred)\n",
    "\n",
    "                # Calculate the log probability of the action we've taken\n",
    "                log_prob = torch.distributions.Categorical(actor_act_pred).log_prob(torch.tensor(action))\n",
    "\n",
    "                # Calculate the entropy/ uncertainty of the policy term. This is used to encourage exploration\n",
    "                entropy = -np.sum(np.mean(detached_act_pred) * np.log(detached_act_pred))\n",
    "                new_state, reward, done, _, _ = self.env.step(action)\n",
    "                steps += 1\n",
    "\n",
    "                state = new_state\n",
    "                \n",
    "                state_tensor = self._obs_to_tensor(state)\n",
    "                if not done:\n",
    "                  q_val_tensor, _ = self.model.forward(state_tensor.float())\n",
    "                else:\n",
    "                    q_val_tensor = torch.tensor(0)\n",
    "                    all_lengths.append(steps)\n",
    "                    average_lengths.append(np.mean(all_lengths[-10:]))\n",
    "                td_error = reward + self.gamma * q_val_tensor - critic_td_error\n",
    "        \n",
    "                # update actor critic\n",
    "                actor_loss = -log_prob * (reward + self.gamma * q_val_tensor.item() - critic_td_error.item())\n",
    "                actor_loss *= i\n",
    "                critic_loss = 0.5 * td_error ** 2\n",
    "                critic_loss *= i\n",
    "\n",
    "                ac_loss = actor_loss + critic_loss + 0.001 * entropy\n",
    "\n",
    "                ac_optimizer.zero_grad()\n",
    "                ac_loss.backward()\n",
    "                ac_optimizer.step()\n",
    "                # all_entropies.append(entropy)\n",
    "                all_losses.append(ac_loss.detach().numpy())\n",
    "                i *= self.gamma\n",
    "            if done:\n",
    "              episode_rewards.append(np.sum(rewards))\n",
    "            self._show_episode_results(episode, steps, state, all_losses, average_lengths)\n",
    "            \n",
    "        return all_lengths, average_lengths, all_entropies, all_losses, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a single agent actor-critic implementation.\n",
    "It is specifically designed to work with the four rooms environment, but it should work with any gym environment.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def obs_to_tensor(obs: np.array) -> torch.Tensor:\n",
    "    # initialize an empty np array\n",
    "    obs_tensor = np.array([])\n",
    "    for _, value in obs.items():\n",
    "        obs_tensor = np.concatenate((obs_tensor, value))\n",
    "    return torch.Tensor(obs_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env\u001b[39m.\u001b[39;49mobservation_space\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticCentralizedMultiAgent(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple, but flexible, implementation of the actor-critic algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, num_actions, actions_per_agent, hidden_size, learning_rate=3e-4):\n",
    "        super(ActorCriticCentralizedMultiAgent, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.actions_per_agent = actions_per_agent\n",
    "        # estimate the value function\n",
    "        self.critic_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.critic_linear2 = nn.Linear(hidden_size, len(self.actions_per_agent))\n",
    "\n",
    "        # estimate the policy distribution\n",
    "        self.actor_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.actor_linear2 = nn.Linear(hidden_size, num_actions)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Do inference to calculate the action probabilities and the state value.\n",
    "        \"\"\"\n",
    "        value = F.relu(self.critic_linear1(state))\n",
    "        value = self.critic_linear2(value)\n",
    "        \n",
    "        policy_dist = F.relu(self.actor_linear1(state))\n",
    "        # softmax effectively generates a probability for each of our output options\n",
    "        policy_dist = self.actor_linear2(policy_dist)\n",
    "        # reshape the policy dist for each actor\n",
    "        policy_dist = policy_dist.view((len(self.actions_per_agent), self.actions_per_agent[0]))\n",
    "\n",
    "        policy_dist = F.softmax(policy_dist, 1)\n",
    "\n",
    "        return value, policy_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CTDNCentralizedMultiAgent(A2CTD0):\n",
    "    def __init__(self, env, obs_size, hidden_size, output_size, actions_per_agent, learning_rate, num_episodes, num_steps, gamma) -> None:\n",
    "        super().__init__(env, obs_size, hidden_size, output_size, learning_rate, num_episodes, num_steps, gamma)\n",
    "        self.actions_per_agent = actions_per_agent\n",
    "\n",
    "        self.model = ActorCriticCentralizedMultiAgent(self.obs_size + 1, self.output_size, actions_per_agent, self.hidden_size, self.learning_rate)\n",
    "    \n",
    "    def _get_actions(self, actor_act_pred, num_agents):\n",
    "        # Sample an action according the probs the network just output.\n",
    "        idx = 0\n",
    "        actions = {}\n",
    "        for agent in env.agents:\n",
    "            action_space_size = env.action_space(agent).n\n",
    "            action = np.random.choice(action_space_size, p=np.squeeze(actor_act_pred[idx]))\n",
    "            actions[agent] = action\n",
    "            idx += 1\n",
    "        return actions\n",
    "\n",
    "    def _show_episode_results(self, episode, steps, state, all_losses, average_lengths, rewards, frequency):\n",
    "        if episode % frequency == 0:\n",
    "            # make an array of the value of each state currently\n",
    "            # make an empty array for the values of each state of shape 10x10\n",
    "            # state_values = np.zeros((10, 10))\n",
    "            # for i in range(1, 11):\n",
    "            #     for j in range(1, 11):\n",
    "            #         state_tensor = self._obs_to_tensor([i, j])\n",
    "            #         value, _ = self.model.forward(state_tensor.float())\n",
    "            #         state_values[i-1][j-1] = value.detach().numpy()\n",
    "            # # show the value of each state with matplot lib\n",
    "            # # plt.imshow(state_values)\n",
    "            # # plt.show()\n",
    "            #     # sys.stdout.write(\"\\nState: \" + str(i) + \" Value: \" + str(state_values))\n",
    "            sys.stdout.write(\"\\nLoss: \" + str(all_losses[-1]) + \"\\nRewards: \" + str(rewards[-1]) + \"\\n\" + \"Average last 20 Rewards\" + str(np.mean(rewards[-20:])))\n",
    "                # + \"\\nCurrent State: \" + str(state))\n",
    "            # Where total length is the number of steps taken in the episode and average length is average steps in all episodes seen \n",
    "            sys.stdout.write(\"\\nepisode: {}, \")\n",
    "            #total length: {}, average length of prev 10: {} \\n\".format(episode, steps, average_lengths[-1]))\n",
    "\n",
    "    def _obs_to_tensor(self, obs: np.array) -> torch.Tensor:\n",
    "        # initialize an empty np array\n",
    "        obs_tensor = np.array([1.0])\n",
    "        for _, value in obs.items():\n",
    "            obs_tensor = np.concatenate((obs_tensor, value))\n",
    "        return torch.Tensor(obs_tensor)\n",
    "\n",
    "    def _combine_rewards(self, rewards):\n",
    "        \"\"\"\n",
    "        Square the rewards, if they are negative initially add a negative sign to the squared value.\n",
    "        Then sum the squares\n",
    "        \"\"\"\n",
    "        return sum([r ** 2 if r > 0 else -(r ** 2) for r in rewards.values()])\n",
    "\n",
    "    def _get_log_probs(self, actor_act_pred, actions):\n",
    "        start = 0\n",
    "        log_probs = []\n",
    "        for agent in env.agents:\n",
    "            action_space_size = env.action_space(agent).n\n",
    "            log_probs.append(torch.distributions.Categorical(actor_act_pred[start: start + action_space_size]).log_prob(torch.tensor(actions[agent])))\n",
    "            start += action_space_size\n",
    "        return torch.concatenate(log_probs)\n",
    "\n",
    "    def train(self):\n",
    "        # Use Adam optimizer for the actor-critic because it should converge faster than SGD and generalization may not be super important\n",
    "        ac_optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        # all episode length\n",
    "        all_lengths = []\n",
    "        episode_rewards = []\n",
    "        # average episode length\n",
    "        average_lengths = []\n",
    "        all_entropies = []\n",
    "        all_losses = []\n",
    "\n",
    "        for episode in range(self.num_episodes):\n",
    "            single_episode_rewards = []\n",
    "            done = False\n",
    "            i = 1\n",
    "            G = 0\n",
    "            state = self.env.reset()\n",
    "            steps = 0\n",
    "            step_rewards = []\n",
    "            states = [state]\n",
    "            dones = []\n",
    "            actions = []\n",
    "\n",
    "            # enable n step actor critic. \n",
    "            while True:\n",
    "                state_tensor = self._obs_to_tensor(state)\n",
    "                _, actor_act_pred = self.model.forward(state_tensor.float())\n",
    "                # drop the tensor dimension and computational graph info\n",
    "                detached_act_pred = actor_act_pred.detach().numpy()\n",
    "                action = self._get_actions(detached_act_pred, len(self.env.agents))\n",
    "\n",
    "                actions.append(action)\n",
    "\n",
    "                # Calculate the entropy/ uncertainty of the policy term. This is used to encourage exploration\n",
    "                # entropy = -np.sum(np.mean(detached_act_pred) * np.log(detached_act_pred))\n",
    "                # entropy_term += entropy\n",
    "                new_state, rewards, done, _, _ = self.env.step(action)\n",
    "                # done is a dict of agents. We only care if all agents are done\n",
    "                done = all(done.values()) or len(self.env.agents) < 2\n",
    "                if done:\n",
    "                    break\n",
    "                steps += 1\n",
    "\n",
    "                state = new_state\n",
    "                step_rewards.append(self._combine_rewards(rewards))\n",
    "                dones.append(done)\n",
    "                states.append(new_state)\n",
    "\n",
    "                if len(step_rewards) >= self.num_steps:\n",
    "                    if len(step_rewards) > self.num_steps:\n",
    "                        step_rewards.pop(0)\n",
    "                        states.pop(0)\n",
    "                        dones.pop(0)\n",
    "                        actions.pop(0)\n",
    "\n",
    "                    # calculate G\n",
    "                    G = 0\n",
    "                    for idx, r in enumerate(step_rewards):\n",
    "                        # account for the dones vector too\n",
    "                        G += r * (self.gamma ** idx) * (1 - dones[idx])\n",
    "                    single_episode_rewards.append(G)\n",
    "\n",
    "                    critic_val, actor_act_pred = self.model.forward(self._obs_to_tensor(states[0]).float())\n",
    "                    if len(env.agents) < 2:\n",
    "                        print('where agent')\n",
    "                    log_probs = self._get_log_probs(actor_act_pred, actions[0])\n",
    "                    next_state_tensor = self._obs_to_tensor(states[-1])\n",
    "                    if not dones[0]:\n",
    "                        next_critic_value_tensor, _ = self.model.forward(next_state_tensor.float())\n",
    "                    else:\n",
    "                        next_critic_value_tensor = torch.zeros(3)\n",
    "                    # calculate the n step td error\n",
    "                    # td error = G + gamma ** self.num_steps * V(s_{t+n}) - V(s_t)\n",
    "                    td_error = G + self.gamma ** self.num_steps * next_critic_value_tensor.T - critic_val.T\n",
    "\n",
    "\n",
    "                    # update actor critic\n",
    "                    actor_loss = -log_probs * (G + self.gamma * next_critic_value_tensor.T.detach() - critic_val.T.detach())\n",
    "                    actor_loss *= i\n",
    "                    critic_loss = 0.5 * td_error ** 2\n",
    "\n",
    "                    ac_loss = actor_loss + critic_loss# + 0.001 * entropy\n",
    "\n",
    "                    ac_optimizer.zero_grad()\n",
    "                    ac_loss.backward(torch.ones_like(ac_loss))\n",
    "                    ac_optimizer.step()\n",
    "                    # all_entropies.append(entropy)\n",
    "                    all_losses.append(ac_loss.detach().numpy())\n",
    "                    i *= self.gamma\n",
    "\n",
    "\n",
    "            all_lengths.append(steps)\n",
    "            average_lengths.append(np.mean(all_lengths[-10:]))\n",
    "            episode_rewards.append(np.sum(single_episode_rewards))\n",
    "            self._show_episode_results(episode, steps, states, all_losses, average_lengths, episode_rewards, 20)\n",
    "            \n",
    "        return all_lengths, average_lengths, all_entropies, all_losses, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: [7.5072326e+08 7.5074707e+08 7.5067213e+08]\n",
      "Rewards: -3582140.91823447\n",
      "Average last 20 Rewards-3582140.91823447\n",
      "episode: {}, \n",
      "Loss: [4630851.5 4578144.5 4675868.5]\n",
      "Rewards: -16277.923836848233\n",
      "Average last 20 Rewards-22311073.074612103\n",
      "episode: {}, \n",
      "Loss: [1.7031249e+10 1.7039343e+10 1.7026545e+10]\n",
      "Rewards: -25644970.351527277\n",
      "Average last 20 Rewards-26604994.800037567\n",
      "episode: {}, \n",
      "Loss: [92793912. 92341032. 93078040.]\n",
      "Rewards: -129081.4231264782\n",
      "Average last 20 Rewards-4491607.552188614\n",
      "episode: {}, \n",
      "Loss: [65637148. 65384148. 65786792.]\n",
      "Rewards: -165522.9019661791\n",
      "Average last 20 Rewards-3101385.235277614\n",
      "episode: {}, \n",
      "Loss: [20367354. 20274122. 20413658.]\n",
      "Rewards: -2362148.546373754\n",
      "Average last 20 Rewards-3128460.223196623\n",
      "episode: {}, \n",
      "Loss: [2.5506493e+09 2.5520645e+09 2.5499843e+09]\n",
      "Rewards: -17621238.742187425\n",
      "Average last 20 Rewards-22935844.20390937\n",
      "episode: {}, \n",
      "Loss: [32084056. 32183230. 31983960.]\n",
      "Rewards: -6746882.193696046\n",
      "Average last 20 Rewards-5589141.839002118\n",
      "episode: {}, "
     ]
    }
   ],
   "source": [
    "hidden_size = 64\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Constants\n",
    "gamma = 0.99\n",
    "max_episodes = 5000\n",
    "num_steps = 5\n",
    "\n",
    "obs_dict = env.reset()\n",
    "obs_size = obs_to_tensor(obs_dict).shape[0]\n",
    "output_size = 0\n",
    "actions_per_agent = []\n",
    "\n",
    "for i in range(len(env.agents)):\n",
    "    output_size += env.action_space(env.agents[i]).n\n",
    "    actions_per_agent.append(env.action_space(list(obs_dict.keys())[i]).n)\n",
    "\n",
    "agent = A2CTDNCentralizedMultiAgent(env, obs_size, hidden_size, output_size, actions_per_agent, learning_rate, num_episodes=max_episodes, num_steps=num_steps, gamma=gamma)\n",
    "\n",
    "all_lengths, average_lengths, all_entropies, all_losses, episode_rewards = agent.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "606dcba7ea3d5e6ab6b9b5ab16aaa6ef92f938b8f3abc4af7f21838acfce664b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
